{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b812dad8",
   "metadata": {},
   "source": [
    "##  Networks and Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041d69c2",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6b032b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile magic_functions.py\n",
    "from tqdm import tqdm\n",
    "from multiprocess import Pool\n",
    "import scipy\n",
    "import networkx as nx\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects import pandas2ri\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pickle\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e373af",
   "metadata": {},
   "outputs": [],
   "source": [
    "### read percentage of organizations in each region and market cap range \n",
    "p_reg = pd.read_excel('C:/Users/Krishna/OneDrive/Documents/IIM_R1/proj2/prob_mats.xlsx', 'reg',index_col=0)\n",
    "p_med = pd.read_excel('C:/Users/Krishna/OneDrive/Documents/IIM_R1/proj2/prob_mats.xlsx', 'med',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3a7532",
   "metadata": {},
   "source": [
    "### Generating network with desired characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90aa306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network(N,nr,er,asa,bs_n,m_size):\n",
    "    \n",
    "    ### Graph generation\n",
    "    ## Total organizations\n",
    "    N=N\n",
    "    ## region specific N\n",
    "    n_regions_list=[int(0.46*N),int( 0.16*N),int( 0.38*N)]\n",
    "    if (len(n_regions_list[0]*[\"NrA\"]+n_regions_list[1]*[\"Eur\"]+n_regions_list[2]*['Asia'])!=N):\n",
    "        if (len(n_regions_list[0]*[\"NrA\"]+n_regions_list[1]*[\"Eur\"]+n_regions_list[2]*['Asia'])-N)>0:\n",
    "            \n",
    "            n_regions_list[0] = n_regions_list[0]+len(n_regions_list[0]*[\"NrA\"]+n_regions_list[1]*[\"Eur\"]+n_regions_list[2]*['Asia'])-N\n",
    "        else:\n",
    "            n_regions_list[0] = n_regions_list[0]-len(n_regions_list[0]*[\"NrA\"]+n_regions_list[1]*[\"Eur\"]+n_regions_list[2]*['Asia'])+N\n",
    "\n",
    "            \n",
    "\n",
    "    g = nx.random_partition_graph(n_regions_list, p_in= 0.60, p_out=0.15, seed=123, directed=True)\n",
    "\n",
    "    edge_list_df=pd.DataFrame(list(g.edges(data=True)))\n",
    "    edge_list_df.columns=['source','target','weight']\n",
    "    \n",
    "    \n",
    "    ###\n",
    "    #calculate n of b,bs,s\n",
    "    nr_n=[int(nr[0]*n_regions_list[0]),int(nr[1]*n_regions_list[0]),int(nr[2]*n_regions_list[0])]\n",
    "    er_n=[int(er[0]*n_regions_list[1]),int(er[1]*n_regions_list[1]),int(er[2]*n_regions_list[1])]\n",
    "    asa_n=[int(asa[0]*n_regions_list[2]),int(asa[1]*n_regions_list[2]),int(asa[2]*n_regions_list[2])]\n",
    "\n",
    "    if (np.sum(nr_n)<n_regions_list[0]):\n",
    "        nr_n[0]=nr_n[0]+(n_regions_list[0]-np.sum(nr_n))\n",
    "    if (np.sum(er_n)<n_regions_list[1]):\n",
    "        er_n[0]=er_n[0]+(n_regions_list[1]-np.sum(er_n))\n",
    "    if (np.sum(asa_n)<n_regions_list[2]):\n",
    "        asa_n[0]=asa_n[0]+(n_regions_list[2]-np.sum(asa_n))\n",
    "    ## if bs n controlled    \n",
    "    k_diff=nr_n[2]-int((nr_n[0]+nr_n[2])/((nr_n[0]/nr_n[2])+1+bs_n))\n",
    "    nr_n[2]=nr_n[2]-k_diff\n",
    "    nr_n[0]=nr_n[0]+k_diff\n",
    "\n",
    "    k_diff=er_n[2]-int((er_n[0]+er_n[2])/((er_n[0]/er_n[2])+1+bs_n))\n",
    "    er_n[2]=er_n[2]-k_diff\n",
    "    er_n[0]=er_n[0]+k_diff\n",
    "\n",
    "    k_diff=asa_n[2]-int((asa_n[0]+asa_n[2])/((asa_n[0]/asa_n[2])+1+bs_n))\n",
    "    asa_n[2]=asa_n[2]-k_diff\n",
    "    asa_n[0]=asa_n[0]+k_diff    \n",
    "\n",
    "    # choose b , s , bs\n",
    "    #nr\n",
    "    list1=range(0,n_regions_list[0])\n",
    "    random.seed(10)\n",
    "    list1_0=random.sample(list1, nr_n[0])\n",
    "    random.seed(10)\n",
    "    list1_1=random.sample(pd.DataFrame(set(list1)-set(list1_0)).iloc[:,0].tolist(),nr_n[1])\n",
    "    random.seed(10)\n",
    "    list1_2=random.sample(pd.DataFrame(set(list1)-(set(list1_1).union(set(list1_0)))).iloc[:,0].tolist(),nr_n[2])\n",
    "\n",
    "    #eur\n",
    "    list2=range(0+n_regions_list[0],n_regions_list[1]+n_regions_list[0])\n",
    "    random.seed(10)\n",
    "    list2_0=random.sample(list2, er_n[0])\n",
    "    random.seed(10)\n",
    "    list2_1=random.sample(pd.DataFrame(set(list2)-set(list2_0)).iloc[:,0].tolist(),er_n[1])\n",
    "    random.seed(10)\n",
    "    list2_2=random.sample(pd.DataFrame(set(list2)-(set(list2_1).union(set(list2_0)))).iloc[:,0].tolist(),er_n[2])\n",
    "\n",
    "    #asi\n",
    "    list3=range(0+n_regions_list[0]+n_regions_list[1],n_regions_list[2]+n_regions_list[0]+n_regions_list[1])\n",
    "    random.seed(10)\n",
    "    list3_0=random.sample(list3, asa_n[0])\n",
    "    random.seed(10)\n",
    "    list3_1=random.sample(pd.DataFrame(set(list3)-set(list3_0)).iloc[:,0].tolist(),asa_n[1])\n",
    "    random.seed(10)\n",
    "    list3_2=random.sample(pd.DataFrame(set(list3)-(set(list3_1).union(set(list3_0)))).iloc[:,0].tolist(),asa_n[2])\n",
    "\n",
    "    #\n",
    "    nodes_frame=pd.DataFrame(range(N),columns=['nodes'])\n",
    "    \n",
    "    nodes_frame['partition']=n_regions_list[0]*[\"NrA\"]+n_regions_list[1]*[\"Eur\"]+n_regions_list[2]*['Asia']\n",
    "\n",
    "    nodes_frame['category']=\"\"\n",
    "\n",
    "    nodes_frame['category'][list1_0]=\"buyer\"\n",
    "    nodes_frame['category'][list2_0]=\"buyer\"\n",
    "    nodes_frame['category'][list3_0]=\"buyer\"\n",
    "\n",
    "    nodes_frame['category'][list1_1]=\"both\"\n",
    "    nodes_frame['category'][list2_1]=\"both\"\n",
    "    nodes_frame['category'][list3_1]=\"both\"\n",
    "\n",
    "    nodes_frame['category'][list1_2]=\"sup\"\n",
    "    nodes_frame['category'][list2_2]=\"sup\"\n",
    "    nodes_frame['category'][list3_2]=\"sup\"\n",
    "    \n",
    "    #\n",
    "    params_sn=pd.read_csv('skew_norm_params_reg_tier_mark_size.csv',index_col=0)\n",
    "    nodes_frame['ms']=\"\"\n",
    "    ########### draw a market size based on region and tier \n",
    "    for i in nodes_frame['nodes']:\n",
    "        ps = params_sn.loc[(params_sn['tier']==nodes_frame['category'][i])&((params_sn['reg']==nodes_frame['partition'][i]))]\n",
    "        #print(ps)\n",
    "        np.random.seed(seed=123)\n",
    "        nodes_frame['ms'][i]  = stats.skewnorm(ps['ae'], ps['loce'], ps['scalee']).rvs(1)[0]\n",
    "\n",
    "    nqn1=np.quantile(nodes_frame['ms'],0.05)\n",
    "    nqn3=np.quantile(nodes_frame['ms'],0.5)\n",
    "    \n",
    "    nodes_frame['ms']=nodes_frame['ms']+ m_size*nodes_frame['ms']\n",
    "    \n",
    "    dummy=pd.DataFrame(columns=['ms'])\n",
    "    dummy['ms']=range(0,N)\n",
    "    \n",
    "    \n",
    "    for i in range(0,N):\n",
    "        if nodes_frame.iloc[i,3]<=nqn1:\n",
    "            dummy['ms'][i]=\"low\"\n",
    "        elif nodes_frame.iloc[i,3]<=nqn3:  \n",
    "            dummy['ms'][i]=\"med\"\n",
    "        else:\n",
    "            dummy['ms'][i]=\"high\"\n",
    "          \n",
    "    nodes_frame['ms2']=dummy['ms']\n",
    "\n",
    "\n",
    "    buy_list=list1_0+list2_0+list3_0\n",
    "    sup_list=list1_2+list2_2+list3_2\n",
    "\n",
    "    edge_list_df_new=edge_list_df.drop([i for i, e in enumerate(list(edge_list_df['source'])) if e in set(sup_list)],axis=0)\n",
    "    new_index=range(edge_list_df_new.shape[0])\n",
    "    edge_list_df_new.index=new_index\n",
    "\n",
    "    edge_list_df_new=edge_list_df_new.drop([i for i, e in enumerate(list(edge_list_df_new['target'])) if e in set(buy_list)],axis=0)\n",
    "    new_index=range(edge_list_df_new.shape[0])\n",
    "    edge_list_df_new.index=new_index\n",
    "    \n",
    "    g = nx.DiGraph( )\n",
    "    # Add edges and edge attributes\n",
    "    for i, elrow in edge_list_df_new.iterrows():\n",
    "        g.add_edge(elrow[0], elrow[1], attr_dict=elrow[2])\n",
    "    \n",
    "    return [edge_list_df_new,nodes_frame,g]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5497d258",
   "metadata": {},
   "source": [
    "### Generate initial attributes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89da47fe",
   "metadata": {},
   "source": [
    "#### Python wrapper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54252e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_lab_attr_all_init(N):\n",
    "    # Defining the R script and loading the instance in Python\n",
    "    r = robjects.r\n",
    "    r['source']('sampling_for_attributes_normal.R')\n",
    "    # Loading the function we have defined in R.\n",
    "    sampling_for_attributes_r2 = robjects.globalenv['sampling_for_attributes_normal']\n",
    "    #Invoking the R function and getting the result\n",
    "    df_result_r = sampling_for_attributes_r2(N)\n",
    "    #Converting it back to a pandas dataframe.\n",
    "    df_result = pandas2ri.rpy2py(df_result_r)\n",
    "    \n",
    "    return(df_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc31866",
   "metadata": {},
   "source": [
    "#### R function for beta distributed tolerance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfad43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(bnlearn)\n",
    "library(stats)\n",
    "sampling_for_attributes_normal <- function(N){\n",
    "  #' Preprocessing df to filter country\n",
    "  #'\n",
    "  #'\n",
    "  data_orgs<-read.csv('C:/Users/Krishna/OneDrive/Documents/IIM_R1/proj2/cosine_input.csv')\n",
    "  library(bnlearn)\n",
    "  library(stats)\n",
    "  my_model <- readRDS(\"C:/Users/Krishna/OneDrive/Documents/IIM_R1/proj2/model_fit.rds\")\n",
    "  x_s <- seq(0, 1, length.out = N)\n",
    "  y1<-dbeta(x_s, 1.1, 0.5)*100\n",
    "  x<-y1\n",
    "  #N_reg=N\n",
    "  for (i in 1:length(x)){\n",
    "    ## S3 method for class 'bn.fit'\n",
    "    sampled_data<-rbn(my_model, n = 500)\n",
    "    sampled_data[,c(1:7)] <- lapply(sampled_data[,c(1:7)], as.numeric)\n",
    "    sampled_data[sampled_data <=0] <- NA\n",
    "    sampled_data[sampled_data >=100] <- NA\n",
    "    r_ind<-rowMeans(sampled_data, na.rm=FALSE) \n",
    "    sampled_data<-sampled_data[!is.na(r_ind),]\n",
    "    #head(sampled_data)\n",
    "    sampled_data$score= as.numeric(rowMeans(sampled_data))#as.matrix(rowMeans(sampled_data))\n",
    "    sc_diffs=abs(x[i]-sampled_data$score)\n",
    "    if(i==1){\n",
    "      sampled_data_f<-sampled_data[sc_diffs==min(sc_diffs),]\n",
    "    }else{\n",
    "      sampled_data_f<-rbind(sampled_data_f,sampled_data[sc_diffs==min(sc_diffs),])\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  return(sampled_data_f)\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a935f1ce",
   "metadata": {},
   "source": [
    "### Generate new attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89496a61",
   "metadata": {},
   "source": [
    "#### Python wrapper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24832bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_lab_attr_new_B(N,reg,s_av1,s_av2):\n",
    "    # Defining the R script and loading the instance in Python\n",
    "    r = robjects.r\n",
    "    r['source']('sampling_for_attributes.R')\n",
    "    # Loading the function we have defined in R.\n",
    "    sampling_for_attributes_r = robjects.globalenv['sampling_for_attributes']\n",
    "    #Invoking the R function and getting the result\n",
    "    df_result_r = sampling_for_attributes_r(N,reg)\n",
    "    #print(df_result_r.head())\n",
    "    #Converting it back to a pandas dataframe.\n",
    "    df_result = pandas2ri.rpy2py(df_result_r)\n",
    "    \"\"\"if (s_av2-s_av1)<2:\n",
    "        s_av2=np.min([s_av2+2,64.28571])\n",
    "        if (s_av2-s_av1)>0:\n",
    "            s_av1=np.max([s_av1-2,0])\n",
    "        else:\n",
    "            s_av1=np.max([s_av2-2,0])\n",
    "            \n",
    "        if s_av2>100:\n",
    "            s_av2=100\"\"\"\n",
    "                        \n",
    "    sampled_data=df_result.loc[((df_result['score']>=(s_av1)) & (df_result['score']<=(s_av2)))]\n",
    "    if sampled_data.shape[0]==0:\n",
    "        s_av=np.mean([s_av1,s_av2])\n",
    "        s_th=s_av*0.05\n",
    "        sampled_data=df_result.loc[((df_result['score']>=(s_av-s_th)) | (df_result['score']<=(s_av+s_th)))]\n",
    "\n",
    "        tmp_vector=np.abs(sampled_data['score']-s_av)\n",
    "        #tmp_vector2=np.abs(df_result['score']-s_av)\n",
    "        sampled_data=sampled_data.loc[tmp_vector==np.min(tmp_vector)]\n",
    "\n",
    "        \n",
    "    return(sampled_data.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7339222a",
   "metadata": {},
   "source": [
    "#### R function to sample new attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcec858",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sampling_for_attributes <- function(N,reg){\n",
    "  #' Preprocessing df to filter country\n",
    "  #'\n",
    "  #'\n",
    "  library(bnlearn)\n",
    "  \n",
    "  #my_model <- readRDS(\"C:/Users/ADMIN/OneDrive/Documents/IIM_R1/proj2/model_fit.rds\")\n",
    "  if ( reg==1){\n",
    "    #C:/Users/ADMIN/OneDrive/Documents\n",
    "    my_model <- readRDS(\"C:/Users/Krishna/OneDrive/Documents/IIM_R1/proj2/model_fit_1.rds\")\n",
    "  }else if(reg==2){\n",
    "    my_model <- readRDS(\"C:/Users/Krishna/OneDrive/Documents/IIM_R1/proj2/model_fit_2.rds\")\n",
    "  }else{\n",
    "    my_model <- readRDS(\"C:/Users/Krishna/OneDrive/Documents/IIM_R1/proj2/model_fit_3.rds\")\n",
    "  }\n",
    "  N_reg=N\n",
    "  N=N+500\n",
    "  ## S3 method for class 'bn.fit'\n",
    "  sampled_data<-rbn(my_model, n = N)\n",
    "  sampled_data[,c(1:7)] <- lapply(sampled_data[,c(1:7)], as.numeric)\n",
    "  sampled_data[sampled_data <=0] <- NA\n",
    "  sampled_data[sampled_data >=100] <- NA\n",
    "  r_ind<-rowMeans(sampled_data, na.rm=FALSE) \n",
    "  sampled_data<-sampled_data[!is.na(r_ind),]\n",
    "  #head(sampled_data)\n",
    "  sampled_data<-sampled_data[sample(nrow(sampled_data), N_reg), ]\n",
    "  rownames(sampled_data) <- seq(length=nrow(sampled_data))\n",
    "  sampled_data\n",
    "  #sampled_data$score= (0.38752934*sampled_data[,1]+ 0.37163856*sampled_data[,2]+ 0.32716766*sampled_data[,3]+ 0.39613783*sampled_data[,4]+ 0.38654069*sampled_data[,5]+0.38654069*sampled_data[,6]+ 0.38589444*sampled_data[,7])/(0.38752934+ 0.37163856+ 0.32716766+ 0.39613783+ 0.38654069+0.38654069+ 0.38589444)\n",
    "  sampled_data$score= as.numeric(rowMeans(sampled_data))#as.matrix(rowMeans(sampled_data))\n",
    "  \n",
    "  return(sampled_data)\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df268c5",
   "metadata": {},
   "source": [
    "### Bayesian Network fit to attributes of an organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddde0c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(bnlearn)\n",
    "\n",
    "\n",
    "data<-read.csv('C:/Users/ADMIN/OneDrive/Documents/IIM_R1/proj2/cosine_input.csv')\n",
    "head(data)\n",
    "data1<-data[data$Region=='North America',]\n",
    "data2<-data[data$Region=='Europe',]\n",
    "data3<-data[data$Region=='Asia',]\n",
    "\n",
    "data<-data[,c(2:8)]\n",
    "data1<-data1[,c(2:8)]\n",
    "data2<-data2[,c(2:8)]\n",
    "data3<-data3[,c(2:8)]\n",
    "\n",
    "dim(data)\n",
    "\n",
    "summary(data)\n",
    "\n",
    "data[,c(1:7)] <- lapply(data[,c(1:7)], as.numeric)\n",
    "data1[,c(1:7)] <- lapply(data1[,c(1:7)], as.numeric)\n",
    "data2[,c(1:7)] <- lapply(data2[,c(1:7)], as.numeric)\n",
    "data3[,c(1:7)] <- lapply(data3[,c(1:7)], as.numeric)\n",
    "\n",
    "\n",
    "bn.scores <- hc(data)\n",
    "bn.scores1 <- hc(data1)\n",
    "bn.scores2 <- hc(data2)\n",
    "bn.scores3 <- hc(data3)\n",
    "\n",
    "plot(bn.scores)\n",
    "plot(bn.scores1)\n",
    "plot(bn.scores2)\n",
    "plot(bn.scores3)\n",
    "\n",
    "bn.scores\n",
    "\n",
    "\n",
    "fit = bn.fit(bn.scores,data )\n",
    "fit1 = bn.fit(bn.scores1,data1 )\n",
    "fit2 = bn.fit(bn.scores2,data2 )\n",
    "fit3 = bn.fit(bn.scores3,data3 )\n",
    "\n",
    "\n",
    "\n",
    "fit\n",
    "\n",
    "\n",
    "bn.fit.qqplot(fit)\n",
    "bn.fit.xyplot(fit)\n",
    "bn.fit.histogram(fit)\n",
    "bn.fit.histogram(fit1)\n",
    "bn.fit.histogram(fit2)\n",
    "bn.fit.histogram(fit3)\n",
    "\n",
    "saveRDS(fit1, file = \"model_fit_1.rds\")\n",
    "saveRDS(fit1, file = \"model_fit_2.rds\")\n",
    "saveRDS(fit1, file = \"model_fit_3.rds\")\n",
    "\n",
    "\n",
    "## S3 method for class 'bn'\n",
    "rbn(bn.scores, n = 1000, data, fit = \"mle\", ..., debug = FALSE)\n",
    "## S3 method for class 'bn.fit'\n",
    "sampled_data<-rbn(fit, n = 1000,)\n",
    "\n",
    "head(sampled_data)\n",
    "\n",
    "write.csv(sampled_data,file = 'C:/Users/ADMIN/OneDrive/Documents/IIM_R1/proj2/sampled_data.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9abca64",
   "metadata": {},
   "source": [
    "### R scripts for cumulative and probability density of a tolerance score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dc1116",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_cdf<-function(cur_sc,reg){\n",
    "  data<-read.csv('C:/Users/Krishna/OneDrive/Documents/IIM_R1/proj2/cosine_input.csv')\n",
    "  #head(data)\n",
    "  reg_lt=unique(data$Region)\n",
    "  \n",
    "  data<-data[data$Region==reg_lt[reg],]\n",
    "  #hist(data$Tolerance,probability=TRUE)\n",
    "  #lines(density(data$Tolerance),col=\"red\")\n",
    "  \n",
    "  ecdff<-ecdf(data$Tolerance)\n",
    "  p=1-ecdff(cur_sc)\n",
    "  return(p)\n",
    "}\n",
    "\n",
    "prob_cdf_m<-function(cur_sc,msh){\n",
    "  data<-read.csv('C:/Users/Krishna/OneDrive/Documents/IIM_R1/proj2/cosine_input.csv')\n",
    "  #head(data)\n",
    "  m_lt=c(unique(data$market_cap)[1],unique(data$market_cap)[3],unique(data$market_cap)[2])\n",
    "  \n",
    "  data<-data[data$market_cap==m_lt[msh],]\n",
    "  #hist(data$Tolerance,probability=TRUE)\n",
    "  #lines(density(data$Tolerance),col=\"red\")\n",
    "  \n",
    "  ecdff<-ecdf(data$Tolerance)\n",
    "  p=1-ecdff(cur_sc)\n",
    "  return(p)\n",
    "}\n",
    "\n",
    "prob_pdf<-function(cur_sc,reg){\n",
    "  #library(MEPDF)\n",
    "  data<-read.csv('C:/Users/Krishna/OneDrive/Documents/IIM_R1/proj2/cosine_input.csv')\n",
    "  #head(data)\n",
    "  reg_lt=unique(data$Region)\n",
    "  \n",
    "  data<-data[data$Region==reg_lt[reg],]\n",
    "  #hist(data$Tolerance,probability=TRUE)\n",
    "  #lines(density(data$Tolerance),col=\"red\")\n",
    "  \n",
    "  #ecdff<-epdf(data$Tolerance)\n",
    "  \n",
    "  kd=density(data$Tolerance)\n",
    "  p= kd$y[which(abs(kd$x-cur_sc)==min(abs(kd$x-cur_sc)))]\n",
    "  return(p)\n",
    "}\n",
    "\n",
    "prob_pdf_m<-function(cur_sc,msh){\n",
    "  #library(MEPDF)\n",
    "  \n",
    "  data<-read.csv('C:/Users/Krishna/OneDrive/Documents/IIM_R1/proj2/cosine_input.csv')\n",
    "  #head(data)\n",
    "  m_lt=c(unique(data$market_cap)[1],unique(data$market_cap)[3],unique(data$market_cap)[2])\n",
    "  \n",
    "  data<-data[data$market_cap==m_lt[msh],]\n",
    "  #hist(data$Tolerance,probability=TRUE)\n",
    "  #lines(density(data$Tolerance),col=\"red\")\n",
    "  \n",
    "  kd=density(data$Tolerance)\n",
    "  p= kd$y[which(abs(kd$x-cur_sc)==min(abs(kd$x-cur_sc)))]\n",
    "  return(p)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f078de6",
   "metadata": {},
   "source": [
    "### Python scripts for running R scripts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9f53c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_cdf(cur_sc,reg):\n",
    "    # Defining the R script and loading the instance in Python\n",
    "    r = robjects.r\n",
    "    r['source']('prob_cdf.R')\n",
    "    # Loading the function we have defined in R.\n",
    "    prob_cdf_r = robjects.globalenv['prob_cdf']\n",
    "    #Invoking the R function and getting the result\n",
    "    df_result_p= prob_cdf_r(cur_sc,reg)\n",
    "    #Converting it back to a pandas dataframe.\n",
    "    #df_result = pandas2ri.rpy2py(df_result_r)\n",
    "    \n",
    "    return(df_result_p)\n",
    "\n",
    "def prob_cdf_m(cur_sc,msh):\n",
    "    # Defining the R script and loading the instance in Python\n",
    "    r = robjects.r\n",
    "    r['source']('prob_cdf.R')\n",
    "    # Loading the function we have defined in R.\n",
    "    prob_cdf_m = robjects.globalenv['prob_cdf_m']\n",
    "    #Invoking the R function and getting the result\n",
    "    df_result_p= prob_cdf_m(cur_sc,msh)\n",
    "    #Converting it back to a pandas dataframe.\n",
    "    #df_result = pandas2ri.rpy2py(df_result_r)\n",
    "    \n",
    "    return(df_result_p)\n",
    "\n",
    "def prob_pdf(cur_sc,reg):\n",
    "    # Defining the R script and loading the instance in Python\n",
    "    r = robjects.r\n",
    "    r['source']('prob_cdf.R')\n",
    "    # Loading the function we have defined in R.\n",
    "    prob_pdf_r = robjects.globalenv['prob_pdf']\n",
    "    #Invoking the R function and getting the result\n",
    "    df_result_p= prob_pdf_r(cur_sc,reg)\n",
    "    #Converting it back to a pandas dataframe.\n",
    "    #df_result = pandas2ri.rpy2py(df_result_r)\n",
    "    \n",
    "    return(df_result_p)\n",
    "def prob_pdf_m(cur_sc,msh):\n",
    "    # Defining the R script and loading the instance in Python\n",
    "    r = robjects.r\n",
    "    r['source']('prob_cdf.R')\n",
    "    # Loading the function we have defined in R.\n",
    "    prob_pdf_m = robjects.globalenv['prob_pdf_m']\n",
    "    #Invoking the R function and getting the result\n",
    "    df_result_p= prob_pdf_m(cur_sc,msh)\n",
    "    #Converting it back to a pandas dataframe.\n",
    "    #df_result = pandas2ri.rpy2py(df_result_r)\n",
    "    \n",
    "    return(df_result_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500a54f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86ec1251",
   "metadata": {},
   "source": [
    "### Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4045342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulation_continous (node_attr,edge_list_df,num_sim,W,bs1,bs2,N,r_on,m_on,p_reg,p_med,probs_mat,probs_mat2,run_iter,alpha1,alpha2,alpha3,Tmp,rgn,mcp):\n",
    "    #nodes and edges\n",
    "    N=N\n",
    "    node_attr = node_attr\n",
    "    edge_list_df = edge_list_df\n",
    "    #P's    \n",
    "    blanck_data_tot=np.empty([N,32,num_sim],dtype='object')\n",
    "    #blanck_data_tot2=np.empty([N,4,num_sim],dtype='object')\n",
    "\n",
    "    for i in tqdm (range (num_sim), desc=\"Running i ...\"):\n",
    "        blanck_data=np.empty([N,32],dtype='object')\n",
    "        #blanck_data2=np.empty([N,4],dtype='object')\n",
    "\n",
    "\n",
    "        # node attr to edge attr\n",
    "        df_3=cosine_similarity(node_attr.iloc[:,:8])\n",
    "        df_4=pd.DataFrame(df_3)\n",
    "        df_4.values[[np.arange(len(df_4))]*2] = np.nan\n",
    "        #mat_data.head()\n",
    "        edge_list_2=df_4.stack().reset_index()\n",
    "        edge_list_2.columns=['source','target','weight']\n",
    "        #edge_list_2.head()\n",
    "        edge_list_f=pd.merge(edge_list_df, edge_list_2,  how='left', left_on=['source','target'], right_on = ['source','target'])\n",
    "        #edge_list_f.head()\n",
    "        edge_list_f.drop('weight_x',axis=1,inplace=True)\n",
    "        edge_list_f.columns=['source','target','weight']\n",
    "        #edge_list_f.head()\n",
    "        st = [\"high\",\"low\"]\n",
    "        \n",
    "        ###########################################################\n",
    "        for j in tqdm (range(0,N), desc=\"Running j...\"):\n",
    "            #N=np.float(N)\n",
    "            if len(list(np.where(edge_list_f.iloc[:,1]==j)[0]))>=1:\n",
    "                ####################################################################################################    \n",
    "                ########################################## MIMETIC##################################################\n",
    "                ####################################################################################################\n",
    "                st=[\"high\",\"low\"]\n",
    "                st=pd.DataFrame(st)\n",
    "                st.columns=['state']\n",
    "\n",
    "                #Index in node attributes df['partitions'] == jth row partition column \n",
    "                p_tier_ind = [i for i, e in enumerate(list(node_attr['tier'])) if e in set([node_attr.iloc[j,10]])]\n",
    "                t_node_attr = node_attr.iloc[p_tier_ind,:]\n",
    "                #t_node_attr=t_node_attr.reset_index().iloc[:,1:]\n",
    "                #t_node_attr.head()\n",
    "\n",
    "\n",
    "\n",
    "                t_node_attr_score=t_node_attr['score'].copy()\n",
    "                t_node_attr_score=t_node_attr_score.reset_index().iloc[:,1:]\n",
    "                #t_node_attr_score\n",
    "\n",
    "                #t_node_attr.index[tnr]\n",
    "\n",
    "                for tnr in range(0,t_node_attr.shape[0]):\n",
    "                    if node_attr.iloc[j,:]['score']<t_node_attr_score['score'][tnr]:\n",
    "                        t_node_attr['state'][t_node_attr.index[tnr]]='high'\n",
    "                    else:\n",
    "                        t_node_attr['state'][t_node_attr.index[tnr]]='low'\n",
    "\n",
    "                tier_p=pd.DataFrame(t_node_attr['state'].value_counts()/np.sum(t_node_attr['state'].value_counts()))\n",
    "                tier_p=tier_p.reset_index()\n",
    "                tier_p.columns=['state','t_p']\n",
    "                #tier_p\n",
    "\n",
    "                t_tier_p=pd.merge(st,tier_p,how=\"left\",left_on=['state'],right_on='state')\n",
    "                t_tier_p=t_tier_p.fillna(0.01)\n",
    "                tier_p=t_tier_p\n",
    "                #tier_p\n",
    "                ###############################################################################       \n",
    "                #d_tier.index\n",
    "\n",
    "                #pd.DataFrame(node_attr.iloc[p_tier_ind,-2-2-1])\n",
    "                #df_4.iloc[list(node_attr.iloc[p_tier_ind,:].index),j].reset_index().iloc[:,-1]\n",
    "\n",
    "                #states and distances \n",
    "                #d_tier=pd.concat([node_attr.iloc[p_tier_ind,-2-2-1],\n",
    "                #           df_4.iloc[list(node_attr.iloc[p_tier_ind,:].index),j] ],axis=1)\n",
    "                d_tier=pd.concat([t_node_attr.iloc[:,-2-2-1],\n",
    "                           df_4.iloc[list(node_attr.iloc[p_tier_ind,:].index),j] ],axis=1)\n",
    "\n",
    "                #print(Ld)\n",
    "                #d_tier=d_tier.drop([j])\n",
    "                #d_tier=d_tier.reset_index()\n",
    "\n",
    "                d_tier=d_tier.fillna(1)\n",
    "\n",
    "                #and average disances per state\n",
    "                d_tier_avg=d_tier.groupby(['state']).mean(str(j))\n",
    "                #d_tier_avg\n",
    "\n",
    "\n",
    "\n",
    "                s_tier_avg=pd.DataFrame(t_node_attr.groupby(['state']).mean()['score'])\n",
    "                s_tier_avg=pd.merge(st,s_tier_avg, how='left', left_on=['state'], right_on = ['state']).fillna(node_attr.iloc[j,:]['score'])\n",
    "                #s_tier_avg\n",
    "\n",
    "                ## state local prob and avg distance\n",
    "                mimetic_p=pd.merge(tier_p,d_tier_avg, how='left', left_on=['state'], right_on = ['state'])\n",
    "                mimetic_p=pd.merge(mimetic_p,s_tier_avg, how='left', left_on=['state'], right_on = ['state'])\n",
    "\n",
    "                #mimetic_p\n",
    "\n",
    "                mimetic_p.columns=['state','tier_p','cur_node','score_m']\n",
    "                mimetic_p['tier_p'] = mimetic_p['tier_p']/np.sum(mimetic_p['tier_p'])\n",
    "                #mimetic_p\n",
    "\n",
    "                #round(mimetic_p['score_m'][0])\n",
    "\n",
    "                ################################################  \n",
    "                region_ind = [i for i, e in enumerate(list(p_reg.columns)) if e in set([node_attr.iloc[j,9]])]\n",
    "                ms_ind = [i for i, e in enumerate(list(p_med.columns)) if e in set([node_attr.iloc[j,12]])]\n",
    "\n",
    "                h_reg=prob_pdf(round(round(mimetic_p['score_m'][0])),region_ind[0]+1)[0]/prob_pdf(round(node_attr.iloc[j,:]['score']),region_ind[0]+1)[0]\n",
    "                l_reg=prob_pdf(round(round(mimetic_p['score_m'][1])),region_ind[0]+1)[0]/prob_pdf(round(node_attr.iloc[j,:]['score']),region_ind[0]+1)[0]\n",
    "                pbreg=pd.DataFrame([h_reg/(h_reg+l_reg),l_reg/(h_reg+l_reg)],columns=['pbreg'])\n",
    "                pbreg.index=['high','low']\n",
    "                pbreg=pbreg.reset_index()\n",
    "                pbreg.columns=['state','pbreg']\n",
    "                #pbreg\n",
    "\n",
    "                h_reg=prob_pdf_m(round(round(mimetic_p['score_m'][0])),ms_ind[0]+1)[0]/prob_pdf_m(round(node_attr.iloc[j,:]['score']),ms_ind[0]+1)[0]\n",
    "                l_reg=prob_pdf_m(round(round(mimetic_p['score_m'][1])),ms_ind[0]+1)[0]/prob_pdf_m(round(node_attr.iloc[j,:]['score']),ms_ind[0]+1)[0]\n",
    "                pbm=pd.DataFrame([h_reg/(h_reg+l_reg),l_reg/(h_reg+l_reg)],columns=['pbm'])\n",
    "                pbm.index=['high','low']\n",
    "                pbm=pbm.reset_index()\n",
    "                pbm.columns=['state','pbm']\n",
    "                #pbm\n",
    "                pbreg.index=mimetic_p.index\n",
    "                pbm.index=mimetic_p.index\n",
    "\n",
    "\n",
    "                mimetic_p['pbreg_m']=pbreg['pbreg']\n",
    "                mimetic_p['pbm_m']=pbm['pbm']\n",
    "                #mimetic_p\n",
    "                ####################################################################################################    \n",
    "                ########################################## Local & Global / inform reg & normative #################\n",
    "                ####################################################################################################\n",
    "                #Index in node attributes df for rows with target column == j\n",
    "                prnt_ind = [i for i, e in enumerate(list(node_attr.index)) if e in set(edge_list_f.loc[edge_list_f.iloc[:,1]==j].iloc[:,0])] \n",
    "                #Index in node attributes df for rows with target column == j\n",
    "                prnt_ind2 = [i for i, e in enumerate(list(node_attr.index)) if e in set(edge_list_f.loc[edge_list_f.iloc[:,0]==j].iloc[:,1])] \n",
    "\n",
    "                l_node_attr = node_attr.iloc[prnt_ind,:]\n",
    "                l_node_attr_score=l_node_attr['score'].copy()\n",
    "                l_node_attr_score=l_node_attr_score.reset_index().iloc[:,1:]\n",
    "\n",
    "\n",
    "                #len(l_node_attr.iloc[:,-2-2-1])\n",
    "\n",
    "                #l_node_attr.loc[j]\n",
    "\n",
    "                for tnr in range(0,l_node_attr.shape[0]):\n",
    "                    if node_attr.iloc[j,:]['score']<l_node_attr_score['score'][tnr]:\n",
    "                        l_node_attr['state'][l_node_attr.index[tnr]]='high'\n",
    "                    else:\n",
    "                        l_node_attr['state'][l_node_attr.index[tnr]]='low'\n",
    "\n",
    "                l2_node_attr = node_attr.iloc[prnt_ind2,:]\n",
    "                l2_node_attr_score=l2_node_attr['score'].copy()\n",
    "                l2_node_attr_score=l2_node_attr_score.reset_index().iloc[:,1:]\n",
    "                for tnr in range(0,l2_node_attr.shape[0]):\n",
    "                    if node_attr.iloc[j,:]['score']<l2_node_attr_score['score'][tnr]:\n",
    "                        l2_node_attr['state'][l2_node_attr.index[tnr]]='high'\n",
    "                    else:\n",
    "                        l2_node_attr['state'][l2_node_attr.index[tnr]]='low'\n",
    "\n",
    "\n",
    "\n",
    "                #Lp1\n",
    "\n",
    "                if len(prnt_ind2)>0:\n",
    "                    #states prob of parent nodes(can also clculate d*count probabilities)\n",
    "                    Lp1 = pd.DataFrame(l_node_attr.iloc[:,-2-2-1].value_counts()/np.sum(l_node_attr.iloc[:,-2-2-1].value_counts()))\n",
    "                    Lp1 = Lp1.reset_index()\n",
    "                    #states prob of parent nodes(can also clculate d*count probabilities)\n",
    "                    Lp2 = pd.DataFrame(l2_node_attr.iloc[:,-2-2-1].value_counts()/np.sum(l2_node_attr.iloc[:,-2-2-1].value_counts()))\n",
    "                    Lp2 = Lp2.reset_index()\n",
    "                    Lp1=pd.merge(st,Lp1,how=\"left\",left_on=['state'],right_on='index').fillna(0.01)\n",
    "                    Lp2=pd.merge(st,Lp2,how=\"left\",left_on=['state'],right_on='index').fillna(0.01)\n",
    "                    Lp=pd.merge(Lp1,Lp2,how=\"left\",left_on=['state_x'],right_on='state_x')\n",
    "                    #print(Lp.head())\n",
    "                    Lp['state']=bs1*Lp['state_y_x']+bs2*Lp['state_y_y']\n",
    "                    Lp=Lp.iloc[:,[0,5]]\n",
    "                    Lp.columns=['index','state']\n",
    "                    #print(Lp1.head())\n",
    "                    #print(Lp2.head())\n",
    "\n",
    "\n",
    "                else:\n",
    "                    #states prob of parent nodes(can also clculate d*count probabilities)\n",
    "                    Lp = pd.DataFrame(l_node_attr.iloc[:,-2-2-1].value_counts()/np.sum(l_node_attr.iloc[:,-2-2-1].value_counts()))\n",
    "                    Lp = Lp.reset_index()\n",
    "                    #print(Lp)\n",
    "                    Lp=pd.merge(st,Lp,how=\"left\",left_on=['state'],right_on='index').fillna(0.01)\n",
    "                    Lp=Lp.iloc[:,[0,2]]\n",
    "                    #print(Lp)\n",
    "                    Lp.columns=['index','state']                \n",
    "\n",
    "                    #Lp.head()\n",
    "\n",
    "                if len(prnt_ind2)>0:\n",
    "\n",
    "                    #states and distances \n",
    "                    Ld1=pd.concat([l_node_attr.iloc[:,-2-2-1],\n",
    "                           df_4.iloc[list(node_attr.iloc[prnt_ind,:].index),j] ],axis=1)\n",
    "\n",
    "                    Lad1=Ld1.groupby(['state']).mean()\n",
    "\n",
    "                    #states and distances \n",
    "                    Ld2=pd.concat([l2_node_attr.iloc[:,-2-2-1],\n",
    "                           df_4.iloc[list(node_attr.iloc[prnt_ind2,:].index),j] ],axis=1)\n",
    "                    #Lp2.head()\n",
    "                    Lad2=Ld2.groupby(['state']).mean()\n",
    "                    Lad1=pd.merge(st,Lad1,how=\"left\",left_on=['state'],right_on='state').fillna(0.01)\n",
    "                    Lad2=pd.merge(st,Lad2,how=\"left\",left_on=['state'],right_on='state').fillna(0.01)\n",
    "                    Lad=pd.merge(Lad1,Lad2,how=\"left\",left_on=['state'],right_on='state').fillna(0.01)\n",
    "                    #print(Lad)\n",
    "                    Lad['state_n']=bs1*Lad[str(j)+'_x']+bs2*Lad[str(j)+'_y']\n",
    "                    Lad=Lad.iloc[:,[0,3]]\n",
    "                    Lad.columns=['state',str(j)]\n",
    "                    Lad.index=Lad['state']\n",
    "                    Lad=Lad.iloc[:,1]\n",
    "                    #print(Lad.head())\n",
    "                    s_l1_avg=pd.DataFrame(l_node_attr.groupby(['state']).mean()['score'])\n",
    "                    s_l1_avg=pd.merge(st,s_l1_avg, how='left', left_on=['state'], right_on = ['state']).fillna(node_attr.iloc[j,:]['score'])\n",
    "                    s_l2_avg=pd.DataFrame(l2_node_attr.groupby(['state']).mean()['score'])\n",
    "                    s_l2_avg=pd.merge(st,s_l2_avg, how='left', left_on=['state'], right_on = ['state']).fillna(node_attr.iloc[j,:]['score'])\n",
    "                    s_l_avg=pd.merge(s_l1_avg,s_l2_avg,how=\"left\",left_on=['state'],right_on='state')\n",
    "                    #print(s_l_avg)\n",
    "                    s_l_avg['score_n']=bs1*s_l_avg['score'+'_x']+bs2*s_l_avg['score'+'_y']\n",
    "                    s_l_avg=s_l_avg.iloc[:,[0,3]]\n",
    "                    s_l_avg.columns=['state','score']\n",
    "\n",
    "                else:\n",
    "                    #states and distances \n",
    "                    Ld=pd.concat([l_node_attr.iloc[:,-2-2-1],\n",
    "                               df_4.iloc[list(node_attr.iloc[prnt_ind,:].index),j] ],axis=1)\n",
    "                    #print(Ld)\n",
    "                    #and average disances per state\n",
    "                    Lad=Ld.groupby(['state']).mean()#str(j)\n",
    "                    Lad=pd.merge(st,Lad,how=\"left\",left_on=['state'],right_on='state').fillna(0.01)\n",
    "\n",
    "                    Lad=Lad.reset_index()\n",
    "\n",
    "                    s_l_avg=pd.DataFrame(l_node_attr.groupby(['state']).mean()['score'])\n",
    "                    s_l_avg=pd.merge(st,s_l_avg, how='left', left_on=['state'], right_on = ['state']).fillna(node_attr.iloc[j,:]['score'])\n",
    "                    s_l_avg=s_l_avg.reset_index()\n",
    "\n",
    "                    #Lad.head()\n",
    "                #print(Lad)\n",
    "\n",
    "                #Lad\n",
    "\n",
    "                #s_l_avg\n",
    "\n",
    "                #print(dist_local)\n",
    "                if len(prnt_ind2)>0:\n",
    "\n",
    "                    ## state local prob and avg distance\n",
    "                    dist_local=pd.merge(Lp,Lad, how='left', left_on=['index'], right_on = ['state'])\n",
    "                    dist_local.columns=['state','local_prob','cur_node_l']\n",
    "                    #dist_local\n",
    "\n",
    "                    dist_local=pd.merge(dist_local,s_l_avg, how='left', left_on=['state'], right_on = ['state'])\n",
    "                else :\n",
    "                    #bs1*s_l_avg['score'+'_x']+s_l_avg*Lad['score'+'_y']\n",
    "                    dist_local=pd.merge(Lp,Lad, how='left', left_on=['index'], right_on = ['state'])\n",
    "                    dist_local=dist_local.iloc[:,[0,1,4]]\n",
    "                    dist_local.columns=['state','local_prob','cur_node_l']\n",
    "                    #dist_local\n",
    "                    #print(s_l_avg)\n",
    "\n",
    "                    dist_local=pd.merge(dist_local,s_l_avg, how='left', left_on=['state'], right_on = ['state'])\n",
    "                    dist_local=dist_local.iloc[:,[0,1,2,4]]\n",
    "\n",
    "\n",
    "\n",
    "                #dist_local\n",
    "\n",
    "                dist_local.columns=['state','local_prob','cur_node_l','score_l']\n",
    "\n",
    "                dist_local['local_prob']=dist_local['local_prob']/np.sum(dist_local['local_prob'])\n",
    "\n",
    "                #print(dist_local)\n",
    "\n",
    "                h_reg=prob_pdf(round(round(dist_local['score_l'][0])),region_ind[0]+1)[0]/prob_pdf(round(node_attr.iloc[j,:]['score']),region_ind[0]+1)[0]\n",
    "                l_reg=prob_pdf(round(round(dist_local['score_l'][1])),region_ind[0]+1)[0]/prob_pdf(round(node_attr.iloc[j,:]['score']),region_ind[0]+1)[0]\n",
    "                pbreg=pd.DataFrame([h_reg/(h_reg+l_reg),l_reg/(h_reg+l_reg)],columns=['pbreg'])\n",
    "                pbreg.index=['high','low']\n",
    "                pbreg=pbreg.reset_index()\n",
    "                pbreg.columns=['state','pbreg']\n",
    "                #pbreg\n",
    "\n",
    "                h_reg=prob_pdf_m(round(round(dist_local['score_l'][0])),ms_ind[0]+1)[0]/prob_pdf_m(round(node_attr.iloc[j,:]['score']),ms_ind[0]+1)[0]\n",
    "                l_reg=prob_pdf_m(round(round(dist_local['score_l'][1])),ms_ind[0]+1)[0]/prob_pdf_m(round(node_attr.iloc[j,:]['score']),ms_ind[0]+1)[0]\n",
    "                pbm=pd.DataFrame([h_reg/(h_reg+l_reg),l_reg/(h_reg+l_reg)],columns=['pbm'])\n",
    "                pbm.index=['high','low']\n",
    "                pbm=pbm.reset_index()\n",
    "                pbm.columns=['state','pbm']\n",
    "                #pbm\n",
    "                pbreg.index=mimetic_p.index\n",
    "                pbm.index=mimetic_p.index\n",
    "\n",
    "\n",
    "                dist_local['pbreg_l']=pbreg['pbreg']\n",
    "                dist_local['pbm_l']=pbm['pbm']\n",
    "                #dist_local\n",
    "\n",
    "                ## global prob\n",
    "                #glb_p=pd.DataFrame(node_attr['state'].value_counts()/np.sum(node_attr['state'].value_counts()))\n",
    "                #glb_p=glb_p.reset_index()\n",
    "                #glb_p.columns=['state','g_p']\n",
    "                st=[\"high\",\"low\"]\n",
    "                st=pd.DataFrame(st)\n",
    "                st.columns=['state']\n",
    "                #Index in node attributes df['partitions'] == jth row partition column \n",
    "                p_region_ind = [i for i, e in enumerate(list(node_attr['partition'])) if e in set([node_attr.iloc[j,9]])]\n",
    "                r_node_attr = node_attr.iloc[p_region_ind,:]\n",
    "\n",
    "                r_node_attr_score=r_node_attr['score'].copy()\n",
    "                r_node_attr_score=r_node_attr_score.reset_index().iloc[:,1:]\n",
    "                for tnr in range(0,r_node_attr.shape[0]):\n",
    "                    if node_attr.iloc[j,:]['score']<r_node_attr_score['score'][tnr]:\n",
    "                        r_node_attr['state'][r_node_attr.index[tnr]]='high'\n",
    "                    else:\n",
    "                        r_node_attr['state'][r_node_attr.index[tnr]]='low'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                glb_p=pd.DataFrame(r_node_attr['state'].value_counts()/np.sum(r_node_attr['state'].value_counts()))\n",
    "\n",
    "\n",
    "                glb_p=glb_p.reset_index()\n",
    "                glb_p.columns=['state','g_p']\n",
    "\n",
    "                t_glb_p=pd.merge(st,glb_p,how=\"left\",left_on=['state'],right_on='state')\n",
    "                t_glb_p=t_glb_p.fillna(0.01)\n",
    "                glb_p=t_glb_p\n",
    "\n",
    "                #print(glb_p)\n",
    "                #states and distances \n",
    "                gd=pd.concat([r_node_attr.iloc[:,-2-2-1],\n",
    "                           df_4.iloc[list(node_attr.iloc[p_region_ind,:].index),j] ],axis=1)\n",
    "                #print(gd)\n",
    "\n",
    "                #and average disances per state\n",
    "                gad=gd.groupby(['state']).mean(str(j))\n",
    "                gad=pd.merge(st,gad,how=\"left\",left_on=['state'],right_on='state')\n",
    "                #gad.reset_index(inplace=True)\n",
    "                #print(gad)\n",
    "                s_g_avg=pd.DataFrame(r_node_attr.groupby(['state']).mean()['score'])\n",
    "                s_g_avg=pd.merge(st,s_g_avg, how='left', left_on=['state'], right_on = ['state']).fillna(node_attr.iloc[j,:]['score'])\n",
    "                #s_g_avg\n",
    "\n",
    "                ## state local prob and avg distance\n",
    "                dist_global=pd.merge(glb_p,gad, how='left', left_on=['state'], right_on = ['state'])\n",
    "                dist_global=pd.merge(dist_global,s_g_avg, how='left', left_on=['state'], right_on = ['state'])\n",
    "\n",
    "                #dist_local\n",
    "\n",
    "                dist_global.columns=['state','glob_prob','cur_node_g','score_g']\n",
    "\n",
    "                dist_global['glob_prob'] =dist_global['glob_prob']/np.sum(dist_global['glob_prob'])\n",
    "                #print(dist_global)\n",
    "\n",
    "                h_reg=prob_pdf(round(round(dist_global['score_g'][0])),region_ind[0]+1)[0]/prob_pdf(round(node_attr.iloc[j,:]['score']),region_ind[0]+1)[0]\n",
    "                l_reg=prob_pdf(round(round(dist_global['score_g'][1])),region_ind[0]+1)[0]/prob_pdf(round(node_attr.iloc[j,:]['score']),region_ind[0]+1)[0]\n",
    "                pbreg=pd.DataFrame([h_reg/(h_reg+l_reg),l_reg/(h_reg+l_reg)],columns=['pbreg'])\n",
    "                pbreg.index=['high','low']\n",
    "                pbreg=pbreg.reset_index()\n",
    "                pbreg.columns=['state','pbreg']\n",
    "                #pbreg\n",
    "\n",
    "                h_reg=prob_pdf_m(round(round(dist_global['score_g'][0])),ms_ind[0]+1)[0]/prob_pdf_m(round(node_attr.iloc[j,:]['score']),ms_ind[0]+1)[0]\n",
    "                l_reg=prob_pdf_m(round(round(dist_global['score_g'][1])),ms_ind[0]+1)[0]/prob_pdf_m(round(node_attr.iloc[j,:]['score']),ms_ind[0]+1)[0]\n",
    "                pbm=pd.DataFrame([h_reg/(h_reg+l_reg),l_reg/(h_reg+l_reg)],columns=['pbm'])\n",
    "                pbm.index=['high','low']\n",
    "                pbm=pbm.reset_index()\n",
    "                pbm.columns=['state','pbm']\n",
    "                #pbm\n",
    "                pbreg.index=mimetic_p.index\n",
    "                pbm.index=mimetic_p.index\n",
    "\n",
    "\n",
    "                dist_global['pbreg_g']=pbreg['pbreg']\n",
    "                dist_global['pbm_g']=pbm['pbm']\n",
    "                #dist_global\n",
    "\n",
    "                #print('glb_p')\n",
    "                if (((i+1)*(j+1)) % 5000) ==0: print(dist_global)\n",
    "                ## all memetic\n",
    "                dist_local_global=pd.merge(dist_global,dist_local, how='left', left_on=['state'], right_on = ['state'])\n",
    "                dist_local_global=dist_local_global.fillna(0.01)\n",
    "\n",
    "                #dist_local_global['m_p']=dist_local_global.product(axis=1)/np.sum(dist_local_global.product(axis=1))\n",
    "                #print(dist_local_global)\n",
    "                #\n",
    "                ####################################################################################################    \n",
    "                ########################################## All_ Pressures ##########################################\n",
    "                ####################################################################################################\n",
    "                #\n",
    "                # All presures\n",
    "                all_p = pd.merge(mimetic_p,dist_local_global,how='left', left_on=['state'], right_on = ['state'])\n",
    "                all_p=all_p.fillna(0.01)\n",
    "                #all_p = pd.merge(all_p,mimetic_p,how='left', left_on=['state'], right_on = ['state'])\n",
    "                #all_p \n",
    "                #= all_p.iloc[:,[0,4,5,6]]\n",
    "\n",
    "                #0.25*all_p.iloc[:,3:5].product(axis=1)\n",
    "\n",
    "                #all_p.iloc[:,3:5]\n",
    "\n",
    "                #w1=w2=w3=w4=0.25\n",
    "                #all_p\n",
    "\n",
    "                #w1*all_p['tier_p'][0]*all_p['cur_node'][0]*all_p['score_m'][0]\n",
    "\n",
    "                #w1=w2=w3=w4=0.25\n",
    "\n",
    "                all_p_tpd=all_p.copy()\n",
    "                #all_p_tpd\n",
    "                all_p_new=pd.DataFrame(all_p_tpd['state'])\n",
    "                #print(all_p_new)\n",
    "                all_p_new['tier_p']=(all_p_tpd['tier_p']*all_p_tpd['cur_node'])/np.sum(all_p_tpd['tier_p']*all_p_tpd['cur_node'])\n",
    "                all_p_new['glob_prob']=(all_p_tpd['glob_prob']*all_p_tpd['cur_node_g'])/np.sum(all_p_tpd['glob_prob']*all_p_tpd['cur_node_g'])\n",
    "                all_p_new['local_prob']=(all_p_tpd['local_prob']*all_p_tpd['cur_node_l'])/np.sum(all_p_tpd['local_prob']*all_p_tpd['cur_node_l'])\n",
    "                all_p_new['pbreg']=(all_p_tpd['pbreg_m']+all_p_tpd['pbreg_g']+all_p_tpd['pbreg_l'])/np.sum(all_p_tpd['pbreg_m']+all_p_tpd['pbreg_g']+all_p_tpd['pbreg_l'])\n",
    "                all_p_new['pbm']=(all_p_tpd['pbm_m']+all_p_tpd['pbm_g']+all_p_tpd['pbm_l'])/np.sum(all_p_tpd['pbm_m']+all_p_tpd['pbm_g']+all_p_tpd['pbm_l'])\n",
    "                all_p_new['score_m']=all_p_tpd['score_m']\n",
    "                all_p_new['score_l']=all_p_tpd['score_l']\n",
    "                all_p_new['score_g']=all_p_tpd['score_g']\n",
    "\n",
    "                #pd.DataFrame(all_p_new)\n",
    "                all_p=all_p_new\n",
    "\n",
    "\n",
    "                \"\"\"\n",
    "                if r_on==1:        \n",
    "                    rpbr =[all_p['pbreg_m'][0],all_p['pbreg_l'][0],all_p['pbreg_g'][0]]\n",
    "                else:\n",
    "                    rpbr =[1,1,1]\n",
    "                if m_on==1:        \n",
    "                    mpbm =[all_p['pbm_m'][0],all_p['pbm_l'][0],all_p['pbm_g'][0]]\n",
    "                else:\n",
    "                    mpbm =[1,1,1]\n",
    "\n",
    "                ptotalh=np.exp((w1*all_p['tier_p'][0]*all_p['cur_node'][0]*rpbr[0]*mpbm[0]*all_p['score_m'][0]+w2*all_p['local_prob'][0]*all_p['cur_node_l'][0]*rpbr[1]*mpbm[1]*all_p['score_l'][0]+w3*all_p['glob_prob'][0]*all_p['cur_node_g'][0]*rpbr[2]*mpbm[2]*all_p['score_g'][0])/w4)/(1+np.exp((w1*all_p['tier_p'][0]*all_p['cur_node'][0]*rpbr[0]*mpbm[0]*all_p['score_m'][0]+w2*all_p['local_prob'][0]*all_p['cur_node_l'][0]*rpbr[1]*mpbm[1]*all_p['score_l'][0]+w3*all_p['glob_prob'][0]*all_p['cur_node_g'][0]*rpbr[2]*mpbm[2]*all_p['score_g'][0])/w4))\n",
    "                ptotall=1-ptotalh\n",
    "                ptot=pd.DataFrame([ptotalh,ptotall],columns=['ptotal'])\n",
    "                ptot.index=all_p.index\n",
    "                all_p['ptotal']=ptot['ptotal']\n",
    "                #all_p\n",
    "                \"\"\"\n",
    "                if r_on==1:        \n",
    "                    rpbr =all_p['pbreg'][0]\n",
    "                else:\n",
    "                    rpbr =0\n",
    "                if m_on==1:        \n",
    "                    mpbm =all_p['pbm'][0]\n",
    "                else:\n",
    "                    mpbm =0\n",
    "                rpmp=(all_p['pbreg']*all_p['pbm'])/np.sum(all_p['pbreg']*all_p['pbm'])\n",
    "                if r_on==0:\n",
    "                    rpmp[0]=1\n",
    "                    rpmp[1]=1\n",
    "                    \n",
    "                ###### multivariate normal\n",
    "                nsd2=list()\n",
    "                for repeat in range(0,100):\n",
    "\n",
    "                    nsd=list()\n",
    "                    for mni in range(0,3):\n",
    "\n",
    "                        nsd.append(np.random.normal(0,1))\n",
    "                    nsd2.append(np.random.multivariate_normal([0]*3,([nsd]*3)))\n",
    "                    #for ni,nsd2i in enumerate(nsd2):\n",
    "                    #    nsd2[ni]=np.round_(nsd2i,2)\n",
    "                nsd2=list(np.round_(pd.DataFrame(nsd2).mean(axis=0),2))\n",
    "                ## 2\n",
    "                if (j==0):\n",
    "                    nsd3=list()\n",
    "                    for repeat in range(0,100):\n",
    "\n",
    "                        nsd=list()\n",
    "                        for mni in range(0,3):\n",
    "\n",
    "                            nsd.append(np.random.normal(0,1))\n",
    "                        nsd3.append(np.random.multivariate_normal([0]*3,([nsd]*3)))\n",
    "                        #for ni,nsd2i in enumerate(nsd2):\n",
    "                        #    nsd2[ni]=np.round_(nsd2i,2)\n",
    "                    nsd3=list(np.round_(pd.DataFrame(nsd3).mean(axis=0),2))\n",
    "                #### normal\n",
    "\n",
    "                epsilon_l=list()\n",
    "                for repeat in range(0,100):\n",
    "                    epsilon_l.append(np.random.normal(0,1))\n",
    "                epsilon=np.mean(epsilon_l) \n",
    "                ####\n",
    "\n",
    "                \"\"\"\n",
    "                if ((node_attr.iloc[j,9]==rgn)&(node_attr.iloc[j,12]==mcp)):\n",
    "                    w1=W[0]\n",
    "                    w2=W[1]\n",
    "                    w3=W[2]\n",
    "                else:\n",
    "                    w1=W[3]\n",
    "                    w2=W[4]\n",
    "                    w3=W[5]\n",
    "                \"\"\"    \n",
    "                ####\n",
    "                if ((node_attr.iloc[j,9]==\"NrA\")&(node_attr.iloc[j,12]==\"high\")):\n",
    "                    w1=W[0]\n",
    "                    w2=W[1]\n",
    "                    w3=W[2]\n",
    "                elif ((node_attr.iloc[j,9]==\"NrA\")&(node_attr.iloc[j,12]==\"high\")):\n",
    "                    w1=W[3]\n",
    "                    w2=W[4]\n",
    "                    w3=W[5]\n",
    "                elif ((node_attr.iloc[j,9]==\"NrA\")&(node_attr.iloc[j,12]==\"high\")):\n",
    "                    w1=W[6]\n",
    "                    w2=W[7]\n",
    "                    w3=W[8]\n",
    "                elif ((node_attr.iloc[j,9]==\"NrA\")&(node_attr.iloc[j,12]==\"high\")):\n",
    "                    w1=W[9]\n",
    "                    w2=W[10]\n",
    "                    w3=W[11]\n",
    "                elif ((node_attr.iloc[j,9]==\"NrA\")&(node_attr.iloc[j,12]==\"high\")):\n",
    "                    w1=W[12]\n",
    "                    w2=W[13]\n",
    "                    w3=W[14]\n",
    "                elif ((node_attr.iloc[j,9]==\"NrA\")&(node_attr.iloc[j,12]==\"high\")):\n",
    "                    w1=W[15]\n",
    "                    w2=W[16]\n",
    "                    w3=W[17]\n",
    "                elif ((node_attr.iloc[j,9]==\"NrA\")&(node_attr.iloc[j,12]==\"high\")):\n",
    "                    w1=W[18]\n",
    "                    w2=W[19]\n",
    "                    w3=W[20]\n",
    "                elif ((node_attr.iloc[j,9]==\"NrA\")&(node_attr.iloc[j,12]==\"high\")):\n",
    "                    w1=W[21]\n",
    "                    w2=W[22]\n",
    "                    w3=W[23]\n",
    "                elif ((node_attr.iloc[j,9]==\"NrA\")&(node_attr.iloc[j,12]==\"high\")):\n",
    "                    w1=W[24]\n",
    "                    w2=W[25]\n",
    "                    w3=W[26]\n",
    "                else :\n",
    "                    w1=0.333\n",
    "                    w2=0.333\n",
    "                    w3=0.333\n",
    "\n",
    "                #ptotalh=np.exp((w1*all_p['tier_p'][0]+w2*all_p['local_prob'][0]+w3*all_p['glob_prob'][0]+w4*rpbr+w5*mpbm))/(1+np.exp((w1*all_p['tier_p'][0]+w2*all_p['local_prob'][0]+w3*all_p['glob_prob'][0]+w4*rpbr+w5*mpbm)))\n",
    "                #ptotalh=((np.exp((w1*(all_p['tier_p'][0]+alpha1)+w2*(all_p['local_prob'][0]+alpha2)+w3*(all_p['glob_prob'][0]+alpha3))/Tmp)/(1+np.exp((w1*(all_p['tier_p'][0]+alpha1)+w2*(all_p['local_prob'][0]+alpha2)+w3*(all_p['glob_prob'][0]+alpha3))/Tmp)))*(rpmp[0]))\n",
    "                ptotalh=((np.exp(((w1+alpha1)*(all_p['tier_p'][0])+(w2+alpha2)*(all_p['local_prob'][0])+(w3+alpha3)*(all_p['glob_prob'][0]) +\n",
    "                                  (nsd2[0]+nsd3[0])*(all_p['tier_p'][0])+(nsd2[1]+nsd3[1])*(all_p['local_prob'][0])+\n",
    "                                  (nsd2[2]+nsd3[2])*(all_p['glob_prob'][0])+epsilon)/Tmp)/(1+np.exp(((w1+alpha1)*(all_p['tier_p'][0])+(w2+alpha2)*(all_p['local_prob'][0])+(w3+alpha3)*(all_p['glob_prob'][0]) +\n",
    "                                  (nsd2[0]+nsd3[0])*(all_p['tier_p'][0])+(nsd2[1]+nsd3[1])*(all_p['local_prob'][0])+\n",
    "                                  (nsd2[2]+nsd3[2])*(all_p['glob_prob'][0])+epsilon)/Tmp)))*(rpmp[0]))\n",
    "\n",
    "                #ptotalh=ptotalh/np.sum(ptotalh)\n",
    "\n",
    "                #ptotall=1-ptotalh\n",
    "                ptotall=(1/(1+np.exp(((w1+alpha1)*(all_p['tier_p'][0])+(w2+alpha2)*(all_p['local_prob'][0])+(w3+alpha3)*(all_p['glob_prob'][0]) +\n",
    "                                  (nsd2[0]+nsd3[0])*(all_p['tier_p'][0])+(nsd2[1]+nsd3[1])*(all_p['local_prob'][0])+\n",
    "                                  (nsd2[2]+nsd3[2])*(all_p['glob_prob'][0])+epsilon)/Tmp)))*(rpmp[1])\n",
    "                \n",
    "                ptot=pd.DataFrame([ptotalh,ptotall],columns=['ptotal'])\n",
    "                ptot.index=all_p.index\n",
    "                all_p['ptotal']=ptot['ptotal']\n",
    "                all_p['ptotal']=all_p['ptotal']/np.sum(all_p['ptotal'])\n",
    "                #all_p\n",
    "\n",
    "\n",
    "                #print(all_p)\n",
    "\n",
    "                #0.6224593312018546\n",
    "                \"\"\"\n",
    "                d_s_ind=np.where(all_p['ptotal']==np.max(all_p['ptotal']))[0][0]\n",
    "                \"\"\"\n",
    "                \n",
    "                if np.count_nonzero([w1,w2,w3])!=0:\n",
    "                    if all_p['ptotal'][0]>0.6224593312018546: \n",
    "                    #0.6224593312018546:\n",
    "                        d_s_ind=0\n",
    "                    elif all_p['ptotal'][0]<0.6224593312018546:\n",
    "                        #0.6224593312018546:\n",
    "                        d_s_ind=1\n",
    "                    else:\n",
    "                        d_s_ind = 1 if np.random.random()<0.5 else 0\n",
    "                else:\n",
    "                    if all_p['ptotal'][0]>0.5:\n",
    "                        d_s_ind=0\n",
    "                    elif all_p['ptotal'][0]<0.5:\n",
    "                        d_s_ind=1\n",
    "                    else:\n",
    "                        d_s_ind = 1 if np.random.random()<0.5 else 0\n",
    "                \"\"\"u = np.random.uniform()\n",
    "                if  all_p['ptotal'][0]>u:\n",
    "                    d_s_ind=0\n",
    "                else:\n",
    "                    d_s_ind=1\"\"\"\n",
    "\n",
    "                #print(d_s_ind)\n",
    "                \"\"\"                                                                                                            \n",
    "                if r_on==1:        \n",
    "                    rpbr =[all_p['pbreg_m'][d_s_ind],all_p['pbreg_l'][d_s_ind],all_p['pbreg_g'][d_s_ind]]\n",
    "                else:\n",
    "                    rpbr =[1,1,1]\n",
    "                if m_on==1:        \n",
    "                    mpbm =[all_p['pbm_m'][d_s_ind],all_p['pbm_l'][d_s_ind],all_p['pbm_g'][d_s_ind]]\n",
    "                else:\n",
    "                    mpbm =[1,1,1]\n",
    "                \"\"\"\n",
    "                if r_on==1:        \n",
    "                    rpbr =all_p['pbreg'][d_s_ind]\n",
    "                else:\n",
    "                    rpbr =0\n",
    "                if m_on==1:        \n",
    "                    mpbm =all_p['pbm'][d_s_ind]\n",
    "                else:\n",
    "                    mpbm =0\n",
    "\n",
    "                \"\"\"s_av=(w1*all_p['tier_p'][d_s_ind]*all_p['cur_node'][d_s_ind]*rpbr[0]*mpbm[0]*all_p['score_m'][d_s_ind]+w2*all_p['local_prob'][d_s_ind]*all_p['cur_node_l'][d_s_ind]*rpbr[1]*mpbm[1]*all_p['score_l'][d_s_ind]+w3*all_p['glob_prob'][d_s_ind]*all_p['cur_node_g'][d_s_ind]*rpbr[2]*mpbm[2]*all_p['score_g'][d_s_ind])/(w1*all_p['tier_p'][d_s_ind]*all_p['cur_node'][d_s_ind]*rpbr[0]*mpbm[0]+w2*all_p['local_prob'][d_s_ind]*all_p['cur_node_l'][d_s_ind]*rpbr[1]*mpbm[1]+w3*all_p['glob_prob'][d_s_ind]*all_p['cur_node_g'][d_s_ind]*rpbr[2]*mpbm[2])\"\"\"\n",
    "                \"\"\"s_av=(w1*all_p['score_m'][d_s_ind]+w2*all_p['score_l'][d_s_ind]+w3*all_p['score_g'][d_s_ind])/(w1+w2+w3)\"\"\"\n",
    "                #    s_av1=np.min([all_p['score_m'][d_s_ind],all_p['score_l'][d_s_ind],all_p['score_g'][d_s_ind]])\n",
    "                #    s_av2=np.max([all_p['score_m'][d_s_ind],all_p['score_l'][d_s_ind],all_p['score_g'][d_s_ind]])\n",
    "\n",
    "                if w1==0:\n",
    "                    s_av1=np.min([all_p['score_l'][d_s_ind],all_p['score_g'][d_s_ind]])\n",
    "                    s_av2=np.max([all_p['score_l'][d_s_ind],all_p['score_g'][d_s_ind]])\n",
    "                elif w2==0:\n",
    "                    s_av1=np.min([all_p['score_m'][d_s_ind],all_p['score_g'][d_s_ind]])\n",
    "                    s_av2=np.max([all_p['score_m'][d_s_ind],all_p['score_g'][d_s_ind]])\n",
    "                elif w3==0:\n",
    "                    s_av1=np.min([all_p['score_m'][d_s_ind],all_p['score_l'][d_s_ind]])\n",
    "                    s_av2=np.max([all_p['score_m'][d_s_ind],all_p['score_l'][d_s_ind]])\n",
    "                elif np.count_nonzero([w1,w2,w3])==1:\n",
    "                    s_av1=[w1*all_p['score_m'][d_s_ind]+w2*all_p['score_l'][d_s_ind]+w3*all_p['score_g'][d_s_ind]]\n",
    "                    s_av2=[w1*all_p['score_m'][d_s_ind]+w2*all_p['score_l'][d_s_ind]+w3*all_p['score_g'][d_s_ind]]\n",
    "                elif np.count_nonzero([w1,w2,w3])==0:\n",
    "                    s_av1=node_attr['score'][j]\n",
    "                    s_av2=node_attr['score'][j]\n",
    "                else:\n",
    "                    \n",
    "                    s_av1=np.min([all_p['score_m'][d_s_ind],all_p['score_l'][d_s_ind],all_p['score_g'][d_s_ind]])\n",
    "                    s_av2=np.max([all_p['score_m'][d_s_ind],all_p['score_l'][d_s_ind],all_p['score_g'][d_s_ind]])\n",
    "\n",
    "                    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                #s_av\n",
    "\n",
    "                #region_ind\n",
    "\n",
    "                #print(all_p)\n",
    "                probs_mat[i,j]=np.max(all_p['ptotal'])\n",
    "                if i==0:\n",
    "                    probs_mat2[i,j]=np.max(all_p['ptotal'])\n",
    "                else:\n",
    "                    probs_mat2[i+j,:]=probs_mat[i,j]\n",
    "\n",
    "                ## hihest prob label\n",
    "                #desired_state = random.choices(list(all_p['state']),list(all_p['all']))[0]\n",
    "                #desired_state = all_p['state'][d_s_ind] \n",
    "                #desired_state\n",
    "                #desired_state = list(all_p.loc[all_p['all']==np.max(all_p['all'])]['state'])[0]\n",
    "\n",
    "\n",
    "\n",
    "                ##### draw attributes with given label\n",
    "\n",
    "                \"\"\"sample_df_1=sample_lab_attr_new(np.float(N),region_ind[0],s_av,0.05*s_av)\"\"\"\n",
    "                \"\"\"if s_av1==s_av2:\n",
    "                    if d_s_ind==0:\n",
    "                    \n",
    "                        sample_df_1=sample_lab_attr_new_B(np.float(N),region_ind[0],s_av1+0.1,100)\n",
    "                    else:\n",
    "                        sample_df_1=sample_lab_attr_new_B(np.float(N),region_ind[0],0,s_av2+0.12)\n",
    "                else:\n",
    "                    sample_df_1=sample_lab_attr_new_B(np.float(N),region_ind[0],s_av1+0.1,s_av2+0.12)\"\"\"\n",
    "                if s_av1==s_av2:\n",
    "                    if d_s_ind==0:\n",
    "                    \n",
    "                        sample_df_1=sample_lab_attr_new_B(np.float(N),region_ind[0],s_av1+0.1,100)\n",
    "                    else:\n",
    "                        sample_df_1=sample_lab_attr_new_B(np.float(N),region_ind[0],0,s_av2+0.12)\n",
    "                else:\n",
    "                    if d_s_ind==0:\n",
    "                    \n",
    "                        sample_df_1=sample_lab_attr_new_B(np.float(N),region_ind[0],(s_av1+s_av2)/2,100)\n",
    "                    else:\n",
    "                        sample_df_1=sample_lab_attr_new_B(np.float(N),region_ind[0],0,(s_av1+s_av2)/2)\n",
    "                    \n",
    "                    \n",
    "                \n",
    "                \n",
    "                ####################################################################################################    \n",
    "                ########################################## Update  attributes ######################################\n",
    "                ####################################################################################################\n",
    "                ## update node attributes \n",
    "                for k,replc in enumerate(sample_df_1.values[0]):\n",
    "                    node_attr.iloc[j,k]=replc                    \n",
    "                ## update edge attributes\n",
    "                # node attr to edge attr\n",
    "                df_3=cosine_similarity(node_attr.iloc[:,:8])\n",
    "                df_4=pd.DataFrame(df_3)\n",
    "                df_4.values[[np.arange(len(df_4))]*2] = np.nan\n",
    "                #mat_data.head()\n",
    "                edge_list_2=df_4.stack().reset_index()\n",
    "                edge_list_2.columns=['source','target','weight']\n",
    "                #edge_list_2.head()\n",
    "                edge_list_f=pd.merge(edge_list_df, edge_list_2,  how='left', left_on=['source','target'], right_on = ['source','target'])\n",
    "                #edge_list_f.head()\n",
    "                edge_list_f.drop('weight_x',axis=1,inplace=True)\n",
    "                edge_list_f.columns=['source','target','weight']\n",
    "\n",
    "\n",
    "\n",
    "                for k,replc in enumerate(node_attr.iloc[j,:].values):\n",
    "                    blanck_data[j,k]=replc \n",
    "\n",
    "                for k,replc in enumerate(all_p.iloc[0,1:].values):\n",
    "                    blanck_data[j,k+13]=replc \n",
    "                blanck_data[j,29]=j\n",
    "                blanck_data[j,30]=i\n",
    "                blanck_data[j,31]=all_p['state'][d_s_ind]\n",
    "\n",
    "\n",
    "                #blanck_data2[:,:2,i]=np.array(edge_list_f) \n",
    "\n",
    "\n",
    "\n",
    "            else:\n",
    "\n",
    "                ####2\n",
    "                ####################################################################################################    \n",
    "                ########################################## MIMETIC##################################################\n",
    "                ####################################################################################################\n",
    "                st=[\"high\",\"low\"]\n",
    "                st=pd.DataFrame(st)\n",
    "                st.columns=['state']\n",
    "\n",
    "                #Index in node attributes df['partitions'] == jth row partition column \n",
    "                p_tier_ind = [i for i, e in enumerate(list(node_attr['tier'])) if e in set([node_attr.iloc[j,10]])]\n",
    "                t_node_attr = node_attr.iloc[p_tier_ind,:]\n",
    "                #t_node_attr=t_node_attr.reset_index().iloc[:,1:]\n",
    "                #t_node_attr.head()\n",
    "\n",
    "\n",
    "\n",
    "                t_node_attr_score=t_node_attr['score'].copy()\n",
    "                t_node_attr_score=t_node_attr_score.reset_index().iloc[:,1:]\n",
    "                #t_node_attr_score\n",
    "\n",
    "                #t_node_attr.index[tnr]\n",
    "\n",
    "                for tnr in range(0,t_node_attr.shape[0]):\n",
    "                    if node_attr.iloc[j,:]['score']<t_node_attr_score['score'][tnr]:\n",
    "                        t_node_attr['state'][t_node_attr.index[tnr]]='high'\n",
    "                    else:\n",
    "                        t_node_attr['state'][t_node_attr.index[tnr]]='low'\n",
    "\n",
    "                tier_p=pd.DataFrame(t_node_attr['state'].value_counts()/np.sum(t_node_attr['state'].value_counts()))\n",
    "                tier_p=tier_p.reset_index()\n",
    "                tier_p.columns=['state','t_p']\n",
    "                #tier_p\n",
    "\n",
    "                t_tier_p=pd.merge(st,tier_p,how=\"left\",left_on=['state'],right_on='state')\n",
    "                t_tier_p=t_tier_p.fillna(0.01)\n",
    "                tier_p=t_tier_p\n",
    "                #tier_p\n",
    "\n",
    "                #d_tier.index\n",
    "\n",
    "                #pd.DataFrame(node_attr.iloc[p_tier_ind,-2-2-1])\n",
    "                #df_4.iloc[list(node_attr.iloc[p_tier_ind,:].index),j].reset_index().iloc[:,-1]\n",
    "\n",
    "                #states and distances \n",
    "                #d_tier=pd.concat([node_attr.iloc[p_tier_ind,-2-2-1],\n",
    "                #           df_4.iloc[list(node_attr.iloc[p_tier_ind,:].index),j] ],axis=1)\n",
    "                d_tier=pd.concat([t_node_attr.iloc[:,-2-2-1],\n",
    "                           df_4.iloc[list(node_attr.iloc[p_tier_ind,:].index),j] ],axis=1)\n",
    "\n",
    "                #print(Ld)\n",
    "                #d_tier=d_tier.drop([j])\n",
    "                #d_tier=d_tier.reset_index()\n",
    "\n",
    "                d_tier=d_tier.fillna(1)\n",
    "\n",
    "                #and average disances per state\n",
    "                d_tier_avg=d_tier.groupby(['state']).mean(str(j))\n",
    "                #d_tier_avg\n",
    "\n",
    "\n",
    "\n",
    "                s_tier_avg=pd.DataFrame(t_node_attr.groupby(['state']).mean()['score'])\n",
    "                s_tier_avg=pd.merge(st,s_tier_avg, how='left', left_on=['state'], right_on = ['state']).fillna(node_attr.iloc[j,:]['score'])\n",
    "                #s_tier_avg\n",
    "\n",
    "                ## state local prob and avg distance\n",
    "                mimetic_p=pd.merge(tier_p,d_tier_avg, how='left', left_on=['state'], right_on = ['state'])\n",
    "                mimetic_p=pd.merge(mimetic_p,s_tier_avg, how='left', left_on=['state'], right_on = ['state'])\n",
    "\n",
    "                #mimetic_p\n",
    "\n",
    "                mimetic_p.columns=['state','tier_p','cur_node','score_m']\n",
    "                mimetic_p['tier_p'] = mimetic_p['tier_p']/np.sum(mimetic_p['tier_p'])\n",
    "                #mimetic_p\n",
    "\n",
    "                #round(mimetic_p['score_m'][0])\n",
    "\n",
    "                ################################################  regulatary mem\n",
    "                region_ind = [i for i, e in enumerate(list(p_reg.columns)) if e in set([node_attr.iloc[j,9]])]\n",
    "                ms_ind = [i for i, e in enumerate(list(p_med.columns)) if e in set([node_attr.iloc[j,12]])]\n",
    "\n",
    "                h_reg=prob_pdf(round(round(mimetic_p['score_m'][0])),region_ind[0]+1)[0]/prob_pdf(round(node_attr.iloc[j,:]['score']),region_ind[0]+1)[0]\n",
    "                l_reg=prob_pdf(round(round(mimetic_p['score_m'][1])),region_ind[0]+1)[0]/prob_pdf(round(node_attr.iloc[j,:]['score']),region_ind[0]+1)[0]\n",
    "                pbreg=pd.DataFrame([h_reg/(h_reg+l_reg),l_reg/(h_reg+l_reg)],columns=['pbreg'])\n",
    "                pbreg.index=['high','low']\n",
    "                pbreg=pbreg.reset_index()\n",
    "                pbreg.columns=['state','pbreg']\n",
    "                #pbreg\n",
    "\n",
    "                h_reg=prob_pdf_m(round(round(mimetic_p['score_m'][0])),ms_ind[0]+1)[0]/prob_pdf_m(round(node_attr.iloc[j,:]['score']),ms_ind[0]+1)[0]\n",
    "                l_reg=prob_pdf_m(round(round(mimetic_p['score_m'][1])),ms_ind[0]+1)[0]/prob_pdf_m(round(node_attr.iloc[j,:]['score']),ms_ind[0]+1)[0]\n",
    "                pbm=pd.DataFrame([h_reg/(h_reg+l_reg),l_reg/(h_reg+l_reg)],columns=['pbm'])\n",
    "                pbm.index=['high','low']\n",
    "                pbm=pbm.reset_index()\n",
    "                pbm.columns=['state','pbm']\n",
    "                #pbm\n",
    "                pbreg.index=mimetic_p.index\n",
    "                pbm.index=mimetic_p.index\n",
    "\n",
    "\n",
    "                mimetic_p['pbreg_m']=pbreg['pbreg']\n",
    "                mimetic_p['pbm_m']=pbm['pbm']\n",
    "                #mimetic_p\n",
    "                ####################################################################################################    \n",
    "                ########################################## Local & Global / inform reg & normative #################\n",
    "                ####################################################################################################\n",
    "                #Index in node attributes df for rows with target column == j\n",
    "                \"\"\"prnt_ind = [i for i, e in enumerate(list(node_attr.index)) if e in set(edge_list_f.loc[edge_list_f.iloc[:,1]==j].iloc[:,0])]\"\"\" \n",
    "                #Index in node attributes df for rows with target column == j\n",
    "                prnt_ind2 = [i for i, e in enumerate(list(node_attr.index)) if e in set(edge_list_f.loc[edge_list_f.iloc[:,0]==j].iloc[:,1])] \n",
    "\n",
    "                \"\"\"l_node_attr = node_attr.iloc[prnt_ind,:]\n",
    "                l_node_attr_score=l_node_attr['score'].copy()\n",
    "                l_node_attr_score=l_node_attr_score.reset_index().iloc[:,1:]\n",
    "\n",
    "\n",
    "                #len(l_node_attr.iloc[:,-2-2-1])\n",
    "\n",
    "                #l_node_attr.loc[j]\n",
    "\n",
    "                for tnr in range(0,l_node_attr.shape[0]):\n",
    "                    if node_attr.iloc[j,:]['score']<l_node_attr_score['score'][tnr]:\n",
    "                        l_node_attr['state'][l_node_attr.index[tnr]]='high'\n",
    "                    else:\n",
    "                        l_node_attr['state'][l_node_attr.index[tnr]]='low'\"\"\"\n",
    "\n",
    "                l2_node_attr = node_attr.iloc[prnt_ind2,:]\n",
    "                l2_node_attr_score=l2_node_attr['score'].copy()\n",
    "                l2_node_attr_score=l2_node_attr_score.reset_index().iloc[:,1:]\n",
    "                for tnr in range(0,l2_node_attr.shape[0]):\n",
    "                    if node_attr.iloc[j,:]['score']<l2_node_attr_score['score'][tnr]:\n",
    "                        l2_node_attr['state'][l2_node_attr.index[tnr]]='high'\n",
    "                    else:\n",
    "                        l2_node_attr['state'][l2_node_attr.index[tnr]]='low'\n",
    "\n",
    "\n",
    "\n",
    "                #Lp1\n",
    "\n",
    "                \"\"\"if len(prnt_ind2)>0:\n",
    "                    #states prob of parent nodes(can also clculate d*count probabilities)\n",
    "                    Lp1 = pd.DataFrame(l_node_attr.iloc[:,-2-2-1].value_counts()/np.sum(l_node_attr.iloc[:,-2-2-1].value_counts()))\n",
    "                    Lp1 = Lp1.reset_index()\n",
    "                    #states prob of parent nodes(can also clculate d*count probabilities)\n",
    "                    Lp2 = pd.DataFrame(l2_node_attr.iloc[:,-2-2-1].value_counts()/np.sum(l2_node_attr.iloc[:,-2-2-1].value_counts()))\n",
    "                    Lp2 = Lp2.reset_index()\n",
    "                    Lp1=pd.merge(st,Lp1,how=\"left\",left_on=['state'],right_on='index').fillna(0.01)\n",
    "                    Lp2=pd.merge(st,Lp2,how=\"left\",left_on=['state'],right_on='index').fillna(0.01)\n",
    "                    Lp=pd.merge(Lp1,Lp2,how=\"left\",left_on=['state_x'],right_on='state_x')\n",
    "                    #print(Lp.head())\n",
    "                    Lp['state']=bs1*Lp['state_y_x']+bs2*Lp['state_y_y']\n",
    "                    Lp=Lp.iloc[:,[0,5]]\n",
    "                    Lp.columns=['index','state']\n",
    "                    #print(Lp1.head())\n",
    "                    #print(Lp2.head())\n",
    "\n",
    "\n",
    "                else:\"\"\"\n",
    "                #states prob of parent nodes(can also clculate d*count probabilities)\n",
    "                Lp = pd.DataFrame(l2_node_attr.iloc[:,-2-2-1].value_counts()/np.sum(l2_node_attr.iloc[:,-2-2-1].value_counts()))\n",
    "                Lp = Lp.reset_index()\n",
    "                #print(Lp)\n",
    "                Lp=pd.merge(st,Lp,how=\"left\",left_on=['state'],right_on='index').fillna(0.01)\n",
    "                Lp=Lp.iloc[:,[0,2]]\n",
    "                #print(Lp)\n",
    "                Lp.columns=['index','state']                \n",
    "                #print(Lp)\n",
    "                \n",
    "\n",
    "                \n",
    "                    #Lp.head()\n",
    "\n",
    "                \"\"\"if len(prnt_ind2)>0:\n",
    "\n",
    "                    #states and distances \n",
    "                    Ld1=pd.concat([l_node_attr.iloc[:,-2-2-1],\n",
    "                           df_4.iloc[list(node_attr.iloc[prnt_ind,:].index),j] ],axis=1)\n",
    "\n",
    "                    Lad1=Ld1.groupby(['state']).mean()\n",
    "\n",
    "                    #states and distances \n",
    "                    Ld2=pd.concat([l2_node_attr.iloc[:,-2-2-1],\n",
    "                           df_4.iloc[list(node_attr.iloc[prnt_ind2,:].index),j] ],axis=1)\n",
    "                    #Lp2.head()\n",
    "                    Lad2=Ld2.groupby(['state']).mean()\n",
    "                    Lad1=pd.merge(st,Lad1,how=\"left\",left_on=['state'],right_on='state').fillna(0.01)\n",
    "                    Lad2=pd.merge(st,Lad2,how=\"left\",left_on=['state'],right_on='state').fillna(0.01)\n",
    "                    Lad=pd.merge(Lad1,Lad2,how=\"left\",left_on=['state'],right_on='state').fillna(0.01)\n",
    "                    #print(Lad)\n",
    "                    Lad['state_n']=bs1*Lad[str(j)+'_x']+bs2*Lad[str(j)+'_y']\n",
    "                    Lad=Lad.iloc[:,[0,3]]\n",
    "                    Lad.columns=['state',str(j)]\n",
    "                    #print(Lad.head())\n",
    "                    s_l1_avg=pd.DataFrame(l_node_attr.groupby(['state']).mean()['score'])\n",
    "                    s_l1_avg=pd.merge(st,s_l1_avg, how='left', left_on=['state'], right_on = ['state']).fillna(node_attr.iloc[j,:]['score'])\n",
    "                    s_l2_avg=pd.DataFrame(l2_node_attr.groupby(['state']).mean()['score'])\n",
    "                    s_l2_avg=pd.merge(st,s_l2_avg, how='left', left_on=['state'], right_on = ['state']).fillna(node_attr.iloc[j,:]['score'])\n",
    "                    s_l_avg=pd.merge(s_l1_avg,s_l2_avg,how=\"left\",left_on=['state'],right_on='state')\n",
    "                    #print(s_l_avg)\n",
    "                    s_l_avg['score_n']=bs1*s_l_avg['score'+'_x']+bs2*s_l_avg['score'+'_y']\n",
    "                    s_l_avg=s_l_avg.iloc[:,[0,3]]\n",
    "                    s_l_avg.columns=['state','score']\n",
    "\n",
    "                else:\"\"\"\n",
    "                #states and distances \n",
    "                Ld=pd.concat([l2_node_attr.iloc[:,-2-2-1],\n",
    "                           df_4.iloc[list(node_attr.iloc[prnt_ind2,:].index),j] ],axis=1)\n",
    "                #print(Ld)\n",
    "                #and average disances per state\n",
    "                Lad=Ld.groupby(['state']).mean()#str(j)\n",
    "                #print(Lad)\n",
    "                Lad=pd.merge(st,Lad,how=\"left\",left_on=['state'],right_on='state').fillna(0.01)\n",
    "                #print(Lad)\n",
    "\n",
    "                Lad=Lad.reset_index()\n",
    "                #print(Lad)\n",
    "\n",
    "                s_l_avg=pd.DataFrame(l2_node_attr.groupby(['state']).mean()['score'])\n",
    "                s_l_avg=pd.merge(st,s_l_avg, how='left', left_on=['state'], right_on = ['state']).fillna(node_attr.iloc[j,:]['score'])\n",
    "                s_l_avg=s_l_avg.reset_index()\n",
    "\n",
    "                    #Lad.head()\n",
    "                #print(Lad)\n",
    "                #Lad.index=Lad['state']\n",
    "                #Lad=Lad.iloc[:,1:]\n",
    "                #print(Lad)\n",
    "                #Lad\n",
    "\n",
    "                #s_l_avg\n",
    "\n",
    "                #bs1*s_l_avg['score'+'_x']+s_l_avg*Lad['score'+'_y']\n",
    "\n",
    "                ## state local prob and avg distance\n",
    "                \n",
    "                dist_local=pd.merge(Lp,Lad, how='left', left_on=['index'], right_on = ['state'])\n",
    "                dist_local=dist_local.iloc[:,[0,1,4]]\n",
    "                dist_local.columns=['state','local_prob','cur_node_l']\n",
    "                #dist_local\n",
    "                #print(s_l_avg)\n",
    "\n",
    "                dist_local=pd.merge(dist_local,s_l_avg, how='left', left_on=['state'], right_on = ['state'])\n",
    "                dist_local=dist_local.iloc[:,[0,1,2,4]]\n",
    "                #print(dist_local)\n",
    "\n",
    "                #dist_local=dist_local.drop(['index'])\n",
    "\n",
    "                #dist_local\n",
    "\n",
    "                dist_local.columns=['state','local_prob','cur_node_l','score_l']\n",
    "\n",
    "                dist_local['local_prob']=dist_local['local_prob']/np.sum(dist_local['local_prob'])\n",
    "\n",
    "                #print(dist_local)\n",
    "\n",
    "                h_reg=prob_pdf(round(round(dist_local['score_l'][0])),region_ind[0]+1)[0]/prob_pdf(round(node_attr.iloc[j,:]['score']),region_ind[0]+1)[0]\n",
    "                l_reg=prob_pdf(round(round(dist_local['score_l'][1])),region_ind[0]+1)[0]/prob_pdf(round(node_attr.iloc[j,:]['score']),region_ind[0]+1)[0]\n",
    "                pbreg=pd.DataFrame([h_reg/(h_reg+l_reg),l_reg/(h_reg+l_reg)],columns=['pbreg'])\n",
    "                pbreg.index=['high','low']\n",
    "                pbreg=pbreg.reset_index()\n",
    "                pbreg.columns=['state','pbreg']\n",
    "                #pbreg\n",
    "\n",
    "                h_reg=prob_pdf_m(round(round(dist_local['score_l'][0])),ms_ind[0]+1)[0]/prob_pdf_m(round(node_attr.iloc[j,:]['score']),ms_ind[0]+1)[0]\n",
    "                l_reg=prob_pdf_m(round(round(dist_local['score_l'][1])),ms_ind[0]+1)[0]/prob_pdf_m(round(node_attr.iloc[j,:]['score']),ms_ind[0]+1)[0]\n",
    "                pbm=pd.DataFrame([h_reg/(h_reg+l_reg),l_reg/(h_reg+l_reg)],columns=['pbm'])\n",
    "                pbm.index=['high','low']\n",
    "                pbm=pbm.reset_index()\n",
    "                pbm.columns=['state','pbm']\n",
    "                #pbm\n",
    "                pbreg.index=mimetic_p.index\n",
    "                pbm.index=mimetic_p.index\n",
    "\n",
    "\n",
    "                dist_local['pbreg_l']=pbreg['pbreg']\n",
    "                dist_local['pbm_l']=pbm['pbm']\n",
    "                #dist_local\n",
    "\n",
    "                ## global prob\n",
    "                #glb_p=pd.DataFrame(node_attr['state'].value_counts()/np.sum(node_attr['state'].value_counts()))\n",
    "                #glb_p=glb_p.reset_index()\n",
    "                #glb_p.columns=['state','g_p']\n",
    "                st=[\"high\",\"low\"]\n",
    "                st=pd.DataFrame(st)\n",
    "                st.columns=['state']\n",
    "                #Index in node attributes df['partitions'] == jth row partition column \n",
    "                p_region_ind = [i for i, e in enumerate(list(node_attr['partition'])) if e in set([node_attr.iloc[j,9]])]\n",
    "                r_node_attr = node_attr.iloc[p_region_ind,:]\n",
    "\n",
    "                r_node_attr_score=r_node_attr['score'].copy()\n",
    "                r_node_attr_score=r_node_attr_score.reset_index().iloc[:,1:]\n",
    "                for tnr in range(0,r_node_attr.shape[0]):\n",
    "                    if node_attr.iloc[j,:]['score']<r_node_attr_score['score'][tnr]:\n",
    "                        r_node_attr['state'][r_node_attr.index[tnr]]='high'\n",
    "                    else:\n",
    "                        r_node_attr['state'][r_node_attr.index[tnr]]='low'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                glb_p=pd.DataFrame(r_node_attr['state'].value_counts()/np.sum(r_node_attr['state'].value_counts()))\n",
    "\n",
    "\n",
    "                glb_p=glb_p.reset_index()\n",
    "                glb_p.columns=['state','g_p']\n",
    "\n",
    "                t_glb_p=pd.merge(st,glb_p,how=\"left\",left_on=['state'],right_on='state')\n",
    "                t_glb_p=t_glb_p.fillna(0.01)\n",
    "                glb_p=t_glb_p\n",
    "\n",
    "                #print(glb_p)\n",
    "                #states and distances \n",
    "                gd=pd.concat([r_node_attr.iloc[:,-2-2-1],\n",
    "                           df_4.iloc[list(node_attr.iloc[p_region_ind,:].index),j] ],axis=1)\n",
    "                #print(gd)\n",
    "\n",
    "                #and average disances per state\n",
    "                gad=gd.groupby(['state']).mean(str(j))\n",
    "                gad=pd.merge(st,gad,how=\"left\",left_on=['state'],right_on='state')\n",
    "                #gad.reset_index(inplace=True)\n",
    "                #print(gad)\n",
    "                s_g_avg=pd.DataFrame(r_node_attr.groupby(['state']).mean()['score'])\n",
    "                s_g_avg=pd.merge(st,s_g_avg, how='left', left_on=['state'], right_on = ['state']).fillna(node_attr.iloc[j,:]['score'])\n",
    "                #s_g_avg\n",
    "\n",
    "                ## state local prob and avg distance\n",
    "                dist_global=pd.merge(glb_p,gad, how='left', left_on=['state'], right_on = ['state'])\n",
    "                dist_global=pd.merge(dist_global,s_g_avg, how='left', left_on=['state'], right_on = ['state'])\n",
    "\n",
    "                #dist_local\n",
    "\n",
    "                dist_global.columns=['state','glob_prob','cur_node_g','score_g']\n",
    "\n",
    "                dist_global['glob_prob'] =dist_global['glob_prob']/np.sum(dist_global['glob_prob'])\n",
    "                #print(dist_global)\n",
    "\n",
    "                h_reg=prob_pdf(round(round(dist_global['score_g'][0])),region_ind[0]+1)[0]/prob_pdf(round(node_attr.iloc[j,:]['score']),region_ind[0]+1)[0]\n",
    "                l_reg=prob_pdf(round(round(dist_global['score_g'][1])),region_ind[0]+1)[0]/prob_pdf(round(node_attr.iloc[j,:]['score']),region_ind[0]+1)[0]\n",
    "                pbreg=pd.DataFrame([h_reg/(h_reg+l_reg),l_reg/(h_reg+l_reg)],columns=['pbreg'])\n",
    "                pbreg.index=['high','low']\n",
    "                pbreg=pbreg.reset_index()\n",
    "                pbreg.columns=['state','pbreg']\n",
    "                #pbreg\n",
    "\n",
    "                h_reg=prob_pdf_m(round(round(dist_global['score_g'][0])),ms_ind[0]+1)[0]/prob_pdf_m(round(node_attr.iloc[j,:]['score']),ms_ind[0]+1)[0]\n",
    "                l_reg=prob_pdf_m(round(round(dist_global['score_g'][1])),ms_ind[0]+1)[0]/prob_pdf_m(round(node_attr.iloc[j,:]['score']),ms_ind[0]+1)[0]\n",
    "                pbm=pd.DataFrame([h_reg/(h_reg+l_reg),l_reg/(h_reg+l_reg)],columns=['pbm'])\n",
    "                pbm.index=['high','low']\n",
    "                pbm=pbm.reset_index()\n",
    "                pbm.columns=['state','pbm']\n",
    "                #pbm\n",
    "                pbreg.index=mimetic_p.index\n",
    "                pbm.index=mimetic_p.index\n",
    "\n",
    "\n",
    "                dist_global['pbreg_g']=pbreg['pbreg']\n",
    "                dist_global['pbm_g']=pbm['pbm']\n",
    "                #dist_global\n",
    "\n",
    "                #print('glb_p')\n",
    "                #if (((i+1)*(j+1)) % 5000) ==0: print(dist_global)\n",
    "                ## all memetic\n",
    "                dist_local_global=pd.merge(dist_global,dist_local, how='left', left_on=['state'], right_on = ['state'])\n",
    "                dist_local_global=dist_local_global.fillna(0.01)\n",
    "\n",
    "                #dist_local_global['m_p']=dist_local_global.product(axis=1)/np.sum(dist_local_global.product(axis=1))\n",
    "                #print(dist_local_global)\n",
    "                #\n",
    "                ####################################################################################################    \n",
    "                ########################################## All_ Pressures ##########################################\n",
    "                ####################################################################################################\n",
    "                #\n",
    "                # All presures\n",
    "                all_p = pd.merge(mimetic_p,dist_local_global,how='left', left_on=['state'], right_on = ['state'])\n",
    "                all_p=all_p.fillna(0.01)\n",
    "                #all_p = pd.merge(all_p,mimetic_p,how='left', left_on=['state'], right_on = ['state'])\n",
    "                #all_p \n",
    "                #= all_p.iloc[:,[0,4,5,6]]\n",
    "\n",
    "                #0.25*all_p.iloc[:,3:5].product(axis=1)\n",
    "\n",
    "                #all_p.iloc[:,3:5]\n",
    "\n",
    "                #w1=w2=w3=w4=0.25\n",
    "                #all_p\n",
    "\n",
    "                #w1*all_p['tier_p'][0]*all_p['cur_node'][0]*all_p['score_m'][0]\n",
    "\n",
    "                #w1=w2=w3=w4=0.25\n",
    "                all_p_tpd=all_p.copy()\n",
    "                #all_p_tpd\n",
    "                all_p_new=pd.DataFrame(all_p_tpd['state'])\n",
    "                #print(all_p_new)\n",
    "                all_p_new['tier_p']=(all_p_tpd['tier_p']*all_p_tpd['cur_node'])/np.sum(all_p_tpd['tier_p']*all_p_tpd['cur_node'])\n",
    "                all_p_new['glob_prob']=(all_p_tpd['glob_prob']*all_p_tpd['cur_node_g'])/np.sum(all_p_tpd['glob_prob']*all_p_tpd['cur_node_g'])\n",
    "                all_p_new['local_prob']=(all_p_tpd['local_prob']*all_p_tpd['cur_node_l'])/np.sum(all_p_tpd['local_prob']*all_p_tpd['cur_node_l'])\n",
    "                all_p_new['pbreg']=(all_p_tpd['pbreg_m']+all_p_tpd['pbreg_g']+all_p_tpd['pbreg_l'])/np.sum(all_p_tpd['pbreg_m']+all_p_tpd['pbreg_g']+all_p_tpd['pbreg_l'])\n",
    "                all_p_new['pbm']=(all_p_tpd['pbm_m']+all_p_tpd['pbm_g']+all_p_tpd['pbm_l'])/np.sum(all_p_tpd['pbm_m']+all_p_tpd['pbm_g']+all_p_tpd['pbm_l'])\n",
    "                all_p_new['score_m']=all_p_tpd['score_m']\n",
    "                all_p_new['score_l']=all_p_tpd['score_l']\n",
    "                all_p_new['score_g']=all_p_tpd['score_g']\n",
    "\n",
    "                #pd.DataFrame(all_p_new)\n",
    "                all_p=all_p_new\n",
    "\n",
    "\n",
    "                \"\"\"\n",
    "                if r_on==1:        \n",
    "                    rpbr =[all_p['pbreg_m'][0],all_p['pbreg_l'][0],all_p['pbreg_g'][0]]\n",
    "                else:\n",
    "                    rpbr =[1,1,1]\n",
    "                if m_on==1:        \n",
    "                    mpbm =[all_p['pbm_m'][0],all_p['pbm_l'][0],all_p['pbm_g'][0]]\n",
    "                else:\n",
    "                    mpbm =[1,1,1]\n",
    "\n",
    "                ptotalh=np.exp((w1*all_p['tier_p'][0]*all_p['cur_node'][0]*rpbr[0]*mpbm[0]*all_p['score_m'][0]+w2*all_p['local_prob'][0]*all_p['cur_node_l'][0]*rpbr[1]*mpbm[1]*all_p['score_l'][0]+w3*all_p['glob_prob'][0]*all_p['cur_node_g'][0]*rpbr[2]*mpbm[2]*all_p['score_g'][0])/w4)/(1+np.exp((w1*all_p['tier_p'][0]*all_p['cur_node'][0]*rpbr[0]*mpbm[0]*all_p['score_m'][0]+w2*all_p['local_prob'][0]*all_p['cur_node_l'][0]*rpbr[1]*mpbm[1]*all_p['score_l'][0]+w3*all_p['glob_prob'][0]*all_p['cur_node_g'][0]*rpbr[2]*mpbm[2]*all_p['score_g'][0])/w4))\n",
    "                ptotall=1-ptotalh\n",
    "                ptot=pd.DataFrame([ptotalh,ptotall],columns=['ptotal'])\n",
    "                ptot.index=all_p.index\n",
    "                all_p['ptotal']=ptot['ptotal']\n",
    "                #all_p\n",
    "                \"\"\"\n",
    "                if r_on==1:        \n",
    "                    rpbr =all_p['pbreg'][0]\n",
    "                else:\n",
    "                    rpbr =0\n",
    "                if m_on==1:        \n",
    "                    mpbm =all_p['pbm'][0]\n",
    "                else:\n",
    "                    mpbm =0\n",
    "                rpmp=(all_p['pbreg']*all_p['pbm'])/np.sum(all_p['pbreg']*all_p['pbm'])\n",
    "                if r_on==0:\n",
    "                    rpmp[0]=1\n",
    "                    rpmp[1]=1\n",
    "                \n",
    "                ###### multivariate normal\n",
    "                nsd2=list()\n",
    "                for repeat in range(0,100):\n",
    "\n",
    "                    nsd=list()\n",
    "                    for mni in range(0,3):\n",
    "\n",
    "                        nsd.append(np.random.normal(0,1))\n",
    "                    nsd2.append(np.random.multivariate_normal([0]*3,([nsd]*3)))\n",
    "                    #for ni,nsd2i in enumerate(nsd2):\n",
    "                    #    nsd2[ni]=np.round_(nsd2i,2)\n",
    "                nsd2=list(np.round_(pd.DataFrame(nsd2).mean(axis=0),2))\n",
    "                ## 2\n",
    "                if (j==0):\n",
    "                    nsd3=list()\n",
    "                    for repeat in range(0,100):\n",
    "\n",
    "                        nsd=list()\n",
    "                        for mni in range(0,3):\n",
    "\n",
    "                            nsd.append(np.random.normal(0,1))\n",
    "                        nsd3.append(np.random.multivariate_normal([0]*3,([nsd]*3)))\n",
    "                        #for ni,nsd2i in enumerate(nsd2):\n",
    "                        #    nsd2[ni]=np.round_(nsd2i,2)\n",
    "                    nsd3=list(np.round_(pd.DataFrame(nsd3).mean(axis=0),2))\n",
    "                #### normal\n",
    "\n",
    "                epsilon_l=list()\n",
    "                for repeat in range(0,100):\n",
    "                    epsilon_l.append(np.random.normal(0,1))\n",
    "                epsilon=np.mean(epsilon_l) \n",
    "\n",
    "\n",
    "                \"\"\"\n",
    "                if ((node_attr.iloc[j,9]==rgn)&(node_attr.iloc[j,12]==mcp)):\n",
    "                    w1=W[0]\n",
    "                    w2=W[1]\n",
    "                    w3=W[2]\n",
    "                else:\n",
    "                    w1=W[3]\n",
    "                    w2=W[4]\n",
    "                    w3=W[5]\n",
    "                \"\"\"    \n",
    "                ####\n",
    "                if ((node_attr.iloc[j,9]==\"NrA\")&(node_attr.iloc[j,12]==\"high\")):\n",
    "                    w1=W[0]\n",
    "                    w2=W[1]\n",
    "                    w3=W[2]\n",
    "                elif ((node_attr.iloc[j,9]==\"NrA\")&(node_attr.iloc[j,12]==\"high\")):\n",
    "                    w1=W[3]\n",
    "                    w2=W[4]\n",
    "                    w3=W[5]\n",
    "                elif ((node_attr.iloc[j,9]==\"NrA\")&(node_attr.iloc[j,12]==\"high\")):\n",
    "                    w1=W[6]\n",
    "                    w2=W[7]\n",
    "                    w3=W[8]\n",
    "                elif ((node_attr.iloc[j,9]==\"NrA\")&(node_attr.iloc[j,12]==\"high\")):\n",
    "                    w1=W[9]\n",
    "                    w2=W[10]\n",
    "                    w3=W[11]\n",
    "                elif ((node_attr.iloc[j,9]==\"NrA\")&(node_attr.iloc[j,12]==\"high\")):\n",
    "                    w1=W[12]\n",
    "                    w2=W[13]\n",
    "                    w3=W[14]\n",
    "                elif ((node_attr.iloc[j,9]==\"NrA\")&(node_attr.iloc[j,12]==\"high\")):\n",
    "                    w1=W[15]\n",
    "                    w2=W[16]\n",
    "                    w3=W[17]\n",
    "                elif ((node_attr.iloc[j,9]==\"NrA\")&(node_attr.iloc[j,12]==\"high\")):\n",
    "                    w1=W[18]\n",
    "                    w2=W[19]\n",
    "                    w3=W[20]\n",
    "                elif ((node_attr.iloc[j,9]==\"NrA\")&(node_attr.iloc[j,12]==\"high\")):\n",
    "                    w1=W[21]\n",
    "                    w2=W[22]\n",
    "                    w3=W[23]\n",
    "                elif ((node_attr.iloc[j,9]==\"NrA\")&(node_attr.iloc[j,12]==\"high\")):\n",
    "                    w1=W[24]\n",
    "                    w2=W[25]\n",
    "                    w3=W[26]\n",
    "                else :\n",
    "                    w1=0.333\n",
    "                    w2=0.333\n",
    "                    w3=0.333\n",
    "\n",
    "                #ptotalh=np.exp((w1*all_p['tier_p'][0]+w2*all_p['local_prob'][0]+w3*all_p['glob_prob'][0]+w4*rpbr+w5*mpbm))/(1+np.exp((w1*all_p['tier_p'][0]+w2*all_p['local_prob'][0]+w3*all_p['glob_prob'][0]+w4*rpbr+w5*mpbm)))\n",
    "                #ptotalh=((np.exp((w1*(all_p['tier_p'][0]+alpha1)+w2*(all_p['local_prob'][0]+alpha2)+w3*(all_p['glob_prob'][0]+alpha3))/Tmp)/(1+np.exp((w1*(all_p['tier_p'][0]+alpha1)+w2*(all_p['local_prob'][0]+alpha2)+w3*(all_p['glob_prob'][0]+alpha3))/Tmp)))*(rpmp[0]))\n",
    "                ptotalh=((np.exp(((w1+alpha1)*(all_p['tier_p'][0])+(w2+alpha2)*(all_p['local_prob'][0])+(w3+alpha3)*(all_p['glob_prob'][0]) +\n",
    "                                  (nsd2[0]+nsd3[0])*(all_p['tier_p'][0])+(nsd2[1]+nsd3[1])*(all_p['local_prob'][0])+\n",
    "                                  (nsd2[2]+nsd3[2])*(all_p['glob_prob'][0])+epsilon)/Tmp)/(1+np.exp(((w1+alpha1)*(all_p['tier_p'][0])+(w2+alpha2)*(all_p['local_prob'][0])+(w3+alpha3)*(all_p['glob_prob'][0]) +\n",
    "                                  (nsd2[0]+nsd3[0])*(all_p['tier_p'][0])+(nsd2[1]+nsd3[1])*(all_p['local_prob'][0])+\n",
    "                                  (nsd2[2]+nsd3[2])*(all_p['glob_prob'][0])+epsilon)/Tmp)))*(rpmp[0]))\n",
    "\n",
    "                #ptotalh=ptotalh/np.sum(ptotalh)\n",
    "\n",
    "                #ptotall=1-ptotalh\n",
    "                ptotall=(1/(1+np.exp(((w1+alpha1)*(all_p['tier_p'][0])+(w2+alpha2)*(all_p['local_prob'][0])+(w3+alpha3)*(all_p['glob_prob'][0]) +\n",
    "                                  (nsd2[0]+nsd3[0])*(all_p['tier_p'][0])+(nsd2[1]+nsd3[1])*(all_p['local_prob'][0])+\n",
    "                                  (nsd2[2]+nsd3[2])*(all_p['glob_prob'][0])+epsilon)/Tmp)))*(rpmp[1])\n",
    "                \n",
    "                ptot=pd.DataFrame([ptotalh,ptotall],columns=['ptotal'])\n",
    "                ptot.index=all_p.index\n",
    "                all_p['ptotal']=ptot['ptotal']\n",
    "                all_p['ptotal']=all_p['ptotal']/np.sum(all_p['ptotal'])\n",
    "                #all_p\n",
    "\n",
    "\n",
    "                #print(all_p)\n",
    "\n",
    "                #0.6224593312018546\n",
    "                \"\"\"\n",
    "                d_s_ind=np.where(all_p['ptotal']==np.max(all_p['ptotal']))[0][0]\n",
    "                \"\"\"\n",
    "                if np.count_nonzero([w1,w2,w3])!=0:\n",
    "                    if all_p['ptotal'][0]>0.6224593312018546:\n",
    "                        #0.6224593312018546:\n",
    "                        d_s_ind=0\n",
    "                    elif all_p['ptotal'][0]<0.6224593312018546:\n",
    "                        #0.6224593312018546:\n",
    "                        d_s_ind=1\n",
    "                    else:\n",
    "                        d_s_ind = 1 if np.random.random()<0.5 else 0\n",
    "                else:\n",
    "                    if all_p['ptotal'][0]>0.5:\n",
    "                        d_s_ind=0\n",
    "                    elif all_p['ptotal'][0]<0.5:\n",
    "                        d_s_ind=1\n",
    "                    else:\n",
    "                        d_s_ind = 1 if np.random.random()<0.5 else 0\n",
    "                \"\"\"u = np.random.uniform()\n",
    "                if  all_p['ptotal'][0]>u:\n",
    "                    d_s_ind=0\n",
    "                else:\n",
    "                    d_s_ind=1\"\"\"\n",
    "                    \n",
    "\n",
    "\n",
    "                #print(d_s_ind)\n",
    "                \"\"\"                                                                                                            \n",
    "                if r_on==1:        \n",
    "                    rpbr =[all_p['pbreg_m'][d_s_ind],all_p['pbreg_l'][d_s_ind],all_p['pbreg_g'][d_s_ind]]\n",
    "                else:\n",
    "                    rpbr =[1,1,1]\n",
    "                if m_on==1:        \n",
    "                    mpbm =[all_p['pbm_m'][d_s_ind],all_p['pbm_l'][d_s_ind],all_p['pbm_g'][d_s_ind]]\n",
    "                else:\n",
    "                    mpbm =[1,1,1]\n",
    "                \"\"\"\n",
    "                if r_on==1:        \n",
    "                    rpbr =all_p['pbreg'][d_s_ind]\n",
    "                else:\n",
    "                    rpbr =0\n",
    "                if m_on==1:        \n",
    "                    mpbm =all_p['pbm'][d_s_ind]\n",
    "                else:\n",
    "                    mpbm =0\n",
    "\n",
    "                \"\"\"s_av=(w1*all_p['tier_p'][d_s_ind]*all_p['cur_node'][d_s_ind]*rpbr[0]*mpbm[0]*all_p['score_m'][d_s_ind]+w2*all_p['local_prob'][d_s_ind]*all_p['cur_node_l'][d_s_ind]*rpbr[1]*mpbm[1]*all_p['score_l'][d_s_ind]+w3*all_p['glob_prob'][d_s_ind]*all_p['cur_node_g'][d_s_ind]*rpbr[2]*mpbm[2]*all_p['score_g'][d_s_ind])/(w1*all_p['tier_p'][d_s_ind]*all_p['cur_node'][d_s_ind]*rpbr[0]*mpbm[0]+w2*all_p['local_prob'][d_s_ind]*all_p['cur_node_l'][d_s_ind]*rpbr[1]*mpbm[1]+w3*all_p['glob_prob'][d_s_ind]*all_p['cur_node_g'][d_s_ind]*rpbr[2]*mpbm[2])\"\"\"\n",
    "                \"\"\"s_av=(w1*all_p['score_m'][d_s_ind]+w2*all_p['score_l'][d_s_ind]+w3*all_p['score_g'][d_s_ind])/(w1+w2+w3)\"\"\"\n",
    "                #    s_av1=np.min([all_p['score_m'][d_s_ind],all_p['score_l'][d_s_ind],all_p['score_g'][d_s_ind]])\n",
    "                #    s_av2=np.max([all_p['score_m'][d_s_ind],all_p['score_l'][d_s_ind],all_p['score_g'][d_s_ind]])\n",
    "\n",
    "                if w1==0:\n",
    "                    s_av1=np.min([all_p['score_l'][d_s_ind],all_p['score_g'][d_s_ind]])\n",
    "                    s_av2=np.max([all_p['score_l'][d_s_ind],all_p['score_g'][d_s_ind]])\n",
    "                elif w2==0:\n",
    "                    s_av1=np.min([all_p['score_m'][d_s_ind],all_p['score_g'][d_s_ind]])\n",
    "                    s_av2=np.max([all_p['score_m'][d_s_ind],all_p['score_g'][d_s_ind]])\n",
    "                elif w3==0:\n",
    "                    s_av1=np.min([all_p['score_m'][d_s_ind],all_p['score_l'][d_s_ind]])\n",
    "                    s_av2=np.max([all_p['score_m'][d_s_ind],all_p['score_l'][d_s_ind]])\n",
    "                elif np.count_nonzero([w1,w2,w3])==1:\n",
    "                    s_av1=[w1*all_p['score_m'][d_s_ind]+w2*all_p['score_l'][d_s_ind]+w3*all_p['score_g'][d_s_ind]]\n",
    "                    s_av2=[w1*all_p['score_m'][d_s_ind]+w2*all_p['score_l'][d_s_ind]+w3*all_p['score_g'][d_s_ind]]\n",
    "                elif np.count_nonzero([w1,w2,w3])==0:\n",
    "                    s_av1=node_attr['score'][j]\n",
    "                    s_av2=node_attr['score'][j]\n",
    "                else:\n",
    "                    \n",
    "                    s_av1=np.min([all_p['score_m'][d_s_ind],all_p['score_l'][d_s_ind],all_p['score_g'][d_s_ind]])\n",
    "                    s_av2=np.max([all_p['score_m'][d_s_ind],all_p['score_l'][d_s_ind],all_p['score_g'][d_s_ind]])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                #s_av\n",
    "\n",
    "                #region_ind\n",
    "\n",
    "                #print(all_p)\n",
    "                probs_mat[i,j]=np.max(all_p['ptotal'])\n",
    "                if i==0:\n",
    "                    probs_mat2[i,j]=np.max(all_p['ptotal'])\n",
    "                else:\n",
    "                    probs_mat2[i+j,:]=probs_mat[i,j]\n",
    "\n",
    "                ## hihest prob label\n",
    "                #desired_state = random.choices(list(all_p['state']),list(all_p['all']))[0]\n",
    "                #desired_state = all_p['state'][d_s_ind] \n",
    "                #desired_state\n",
    "                #desired_state = list(all_p.loc[all_p['all']==np.max(all_p['all'])]['state'])[0]\n",
    "\n",
    "\n",
    "\n",
    "                ##### draw attributes with given label\n",
    "\n",
    "                \"\"\"sample_df_1=sample_lab_attr_new(np.float(N),region_ind[0],s_av,0.05*s_av)\"\"\"\n",
    "                \"\"\"if s_av1==s_av2:\n",
    "                    if d_s_ind==0:\n",
    "                    \n",
    "                        sample_df_1=sample_lab_attr_new_B(np.float(N),region_ind[0],s_av1+0.1,100)\n",
    "                    else:\n",
    "                        sample_df_1=sample_lab_attr_new_B(np.float(N),region_ind[0],0,s_av2+0.12)\n",
    "                else:\n",
    "                    sample_df_1=sample_lab_attr_new_B(np.float(N),region_ind[0],s_av1+0.1,s_av2+0.12)\"\"\"\n",
    "                if s_av1==s_av2:\n",
    "                    if d_s_ind==0:\n",
    "                    \n",
    "                        sample_df_1=sample_lab_attr_new_B(np.float(N),region_ind[0],s_av1+0.1,100)\n",
    "                    else:\n",
    "                        sample_df_1=sample_lab_attr_new_B(np.float(N),region_ind[0],0,s_av2+0.12)\n",
    "                else:\n",
    "                    if d_s_ind==0:\n",
    "                    \n",
    "                        sample_df_1=sample_lab_attr_new_B(np.float(N),region_ind[0],(s_av1+s_av2)/2,100)\n",
    "                    else:\n",
    "                        sample_df_1=sample_lab_attr_new_B(np.float(N),region_ind[0],0,(s_av1+s_av2)/2)\n",
    "                    \n",
    "                    \n",
    "                \n",
    "                ####################################################################################################    \n",
    "                ########################################## Update  attributes ######################################\n",
    "                ####################################################################################################\n",
    "                ## update node attributes \n",
    "                for k,replc in enumerate(sample_df_1.values[0]):\n",
    "                    node_attr.iloc[j,k]=replc                    \n",
    "                ## update edge attributes\n",
    "                # node attr to edge attr\n",
    "                df_3=cosine_similarity(node_attr.iloc[:,:8])\n",
    "                df_4=pd.DataFrame(df_3)\n",
    "                df_4.values[[np.arange(len(df_4))]*2] = np.nan\n",
    "                #mat_data.head()\n",
    "                edge_list_2=df_4.stack().reset_index()\n",
    "                edge_list_2.columns=['source','target','weight']\n",
    "                #edge_list_2.head()\n",
    "                edge_list_f=pd.merge(edge_list_df, edge_list_2,  how='left', left_on=['source','target'], right_on = ['source','target'])\n",
    "                #edge_list_f.head()\n",
    "                edge_list_f.drop('weight_x',axis=1,inplace=True)\n",
    "                edge_list_f.columns=['source','target','weight']\n",
    "\n",
    "\n",
    "\n",
    "                for k,replc in enumerate(node_attr.iloc[j,:].values):\n",
    "                    blanck_data[j,k]=replc \n",
    "\n",
    "                for k,replc in enumerate(all_p.iloc[0,1:].values):\n",
    "                    blanck_data[j,k+13]=replc \n",
    "                blanck_data[j,29]=j\n",
    "                blanck_data[j,30]=i\n",
    "                blanck_data[j,31]=all_p['state'][d_s_ind] \n",
    "\n",
    "                #blanck_data2[:,:2,i]=np.array(edge_list_f) \n",
    "\n",
    "        blanck_data_tot[:,:,i]=pd.DataFrame(blanck_data)\n",
    "\n",
    "                \n",
    "\n",
    "#if i>= 2:\n",
    "    #if i%5==0:\n",
    "        #probs_mat_pr.append(np.prod(np.log(probs_mat[i,:]),axis=1))\n",
    "        \n",
    "        edge_list_f.to_csv(folder_location+\"sc_\"+str(run_iter+1)+\"/\"+str(i)+\"_edge_attr.csv\")\n",
    "    reshaped_bd = np.vstack(blanck_data_tot[:,:,i] for i in range(num_sim))\n",
    "    reshaped_bd_df=pd.DataFrame(reshaped_bd)\n",
    "    reshaped_bd_df.to_csv(folder_location+\"sc_\"+str(run_iter+1)+\"/\"+ \"other_node_attr.csv\")\n",
    "\n",
    "            \n",
    "    print('Complete')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fabc5a2",
   "metadata": {},
   "source": [
    "### Parallelization of simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ded242e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting magic_functions.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def process_func(run_iter):\n",
    "    #print('@@@@@ run iter @@@@@ --' + str(run_iter))\n",
    "    \n",
    "    stc=scn_params.iloc[run_iter,20]\n",
    "    Tmp=scn_params.iloc[run_iter,21]\n",
    "    if stc==1:\n",
    "        nr=[0.22,0.35,0.43]\n",
    "        er=[0.38,0.13,0.50]\n",
    "        asa=[0.22,0.06,0.72]\n",
    "    else:\n",
    "        nr=[0.33,0.335,0.335]\n",
    "        er=[0.33,0.335,0.335]\n",
    "        asa=[0.33,0.335,0.335]\n",
    "        \n",
    "    #W=[scn_params.iloc[run_iter,5],scn_params.iloc[run_iter,6],scn_params.iloc[run_iter,7],scn_params.iloc[run_iter,8],scn_params.iloc[run_iter,9],scn_params.iloc[run_iter,10]]\n",
    "    W=[scn_params.iloc[run_iter,23],scn_params.iloc[run_iter,24],scn_params.iloc[run_iter,25],scn_params.iloc[run_iter,26],\n",
    "       scn_params.iloc[run_iter,27],scn_params.iloc[run_iter,28],scn_params.iloc[run_iter,29],scn_params.iloc[run_iter,30],\n",
    "      scn_params.iloc[run_iter,31],scn_params.iloc[run_iter,32],scn_params.iloc[run_iter,33],scn_params.iloc[run_iter,34],\n",
    "      scn_params.iloc[run_iter,35],scn_params.iloc[run_iter,36],scn_params.iloc[run_iter,37],scn_params.iloc[run_iter,38],\n",
    "      scn_params.iloc[run_iter,39],scn_params.iloc[run_iter,40],scn_params.iloc[run_iter,41],scn_params.iloc[run_iter,42],\n",
    "      scn_params.iloc[run_iter,43],scn_params.iloc[run_iter,44],scn_params.iloc[run_iter,45],scn_params.iloc[run_iter,46],\n",
    "      scn_params.iloc[run_iter,47],scn_params.iloc[run_iter,48],scn_params.iloc[run_iter,49]]\n",
    "\n",
    "    N=scn_params.iloc[run_iter,0]\n",
    "    bs_n=scn_params.iloc[run_iter,3]\n",
    "    m_size=scn_params.iloc[run_iter,4]\n",
    "    bs1=scn_params.iloc[run_iter,1]\n",
    "    bs2=scn_params.iloc[run_iter,2]\n",
    "    rgn=scn_params.iloc[run_iter,13]\n",
    "    mcp=scn_params.iloc[run_iter,14]\n",
    "    \n",
    "    ################################################### create network\n",
    "    network_created=create_network(N,nr,er,asa,bs_n,m_size)\n",
    "    #graph\n",
    "    g=network_created[2]\n",
    "    #centrality\n",
    "    deg_cent = nx.degree_centrality(g)\n",
    "    in_deg_cent = nx.in_degree_centrality(g)\n",
    "    out_deg_cent = nx.out_degree_centrality(g)\n",
    "    eigen_cent = nx.eigenvector_centrality(g)\n",
    "    #katz_cent = nx.katz_centrality(g)\n",
    "    closeness_cent = nx.closeness_centrality(g)\n",
    "    #betw_cent = nx.betweenness_centrality(g)\n",
    "    #vote_cent = nx.voterank(g)\n",
    "    deg=pd.DataFrame(list(deg_cent.values()),columns=['deg'])\n",
    "    indeg=pd.DataFrame(list(in_deg_cent.values()),columns=['indeg'])\n",
    "    outdeg=pd.DataFrame(list(out_deg_cent.values()),columns=['outdeg'])\n",
    "    eigencent=pd.DataFrame(list(eigen_cent.values()),columns=['eigdeg'])\n",
    "    closeness=pd.DataFrame(list(closeness_cent.values()),columns=['closedeg'])\n",
    "    all_net_p=pd.concat([deg,indeg,outdeg,closeness,eigencent],axis=1)\n",
    "    #tier and ms\n",
    "    nodes_frame=network_created[1]\n",
    "    #edge list\n",
    "    edge_list_df_new=network_created[0]\n",
    "    edge_list_df=edge_list_df_new.copy()\n",
    "\n",
    "\n",
    "    n_regions_list=[int(0.46*N),int( 0.16*N),int( 0.38*N)]\n",
    "    if (len(n_regions_list[0]*[\"NrA\"]+n_regions_list[1]*[\"Eur\"]+n_regions_list[2]*['Asia'])!=N):\n",
    "        if (len(n_regions_list[0]*[\"NrA\"]+n_regions_list[1]*[\"Eur\"]+n_regions_list[2]*['Asia'])-N)>0:\n",
    "\n",
    "            n_regions_list[0] = n_regions_list[0]+len(n_regions_list[0]*[\"NrA\"]+n_regions_list[1]*[\"Eur\"]+n_regions_list[2]*['Asia'])-N\n",
    "        else:\n",
    "            n_regions_list[0] = n_regions_list[0]-len(n_regions_list[0]*[\"NrA\"]+n_regions_list[1]*[\"Eur\"]+n_regions_list[2]*['Asia'])+N\n",
    "\n",
    "    #print(n_regions_list)\n",
    "    #till partition\n",
    "    ###################################################initial attributes at random one at a time\n",
    "    #### method 1\n",
    "    \n",
    "\n",
    "    \"\"\"init_samples = sample_lab_attr_all_init(np.float(N),30,10)\"\"\"\n",
    "    init_samples = sample_lab_attr_all_init(np.float(N))\n",
    "    #### method 2\n",
    "    \"\"\"init_samples1 = initial_random_attr(n_regions_list[0],np.array([0.2,0.3,0.5]))\n",
    "    init_samples1=init_samples1.reset_index().iloc[:,1:]\n",
    "    init_samples2 = initial_random_attr(n_regions_list[1],np.array([0.3,0.3,0.4]))\n",
    "    init_samples2=init_samples2.reset_index().iloc[:,1:]\n",
    "    init_samples3 = initial_random_attr(n_regions_list[2],np.array([0.5,0.3,0.2]))\n",
    "    init_samples3=init_samples3.reset_index().iloc[:,1:]\n",
    "\n",
    "    init_samples=pd.concat([init_samples1,init_samples2,init_samples3],axis=0)\"\"\"\n",
    "    \n",
    "    ### method 3\n",
    "    \"\"\"init_samples1 = sample_lab_attr_all(n_regions_list[0],1)\n",
    "    init_samples1=init_samples1.reset_index().iloc[:,1:]\n",
    "    init_samples2 = sample_lab_attr_all(n_regions_list[1],2)\n",
    "    init_samples2=init_samples2.reset_index().iloc[:,1:]\n",
    "    init_samples3 = sample_lab_attr_all(n_regions_list[2],3)\n",
    "    init_samples3=init_samples3.reset_index().iloc[:,1:]\n",
    "\n",
    "    init_samples=pd.concat([init_samples1,init_samples2,init_samples3],axis=0)\"\"\"\n",
    "    ############\n",
    "    \n",
    "    #init_samples.head()\n",
    "    init_samples=init_samples.reset_index()\n",
    "    #init_samples.head()\n",
    "    init_samples=init_samples.iloc[:,1:]\n",
    "    #init_samples.index\n",
    "    node_attr=init_samples\n",
    "    node_attr['state']=\"high\"\n",
    "\n",
    "    node_attr['partition']=\"\"\n",
    "    for i in range(0,node_attr.shape[0]):\n",
    "        if i<n_regions_list[0]:\n",
    "            node_attr['partition'][i]='NrA'\n",
    "        elif i< (n_regions_list[0]+n_regions_list[1]):\n",
    "            node_attr['partition'][i]='Eur'\n",
    "        else:\n",
    "            node_attr['partition'][i]='Asia'\n",
    "\n",
    "    #tier and MS merge with attributes\n",
    "    node_attr = pd.concat([node_attr,nodes_frame.iloc[:,2:]],axis=1)\n",
    "    #node_attr.columns\n",
    "\n",
    "    node_attr.columns=['X1..Commitment...Governance', 'X2..Traceability.and.Risk.Assessment',\n",
    "           'X3..Purchasing.Practices', 'X4..Recruitment', 'X5..Worker.Voice',\n",
    "           'X6..Monitoring', 'X7..Remedy', 'score', 'state', 'partition','tier','ms','ms2']\n",
    "    #\n",
    "\n",
    "    #node_attr.info()\n",
    "\n",
    "\n",
    "\n",
    "    # region wise reg assumption and market size assumption\n",
    "    #p_reg_org=p_reg.copy()\n",
    "    #p_med_org=p_med.copy()\n",
    "\n",
    "    #\n",
    "    init_node_attrs_df=node_attr.copy()\n",
    "    init_edge_attrs_df=edge_list_df.copy()\n",
    "\n",
    "    import os\n",
    "    os.mkdir(folder_location+\"sc_\"+str(run_iter+1))\n",
    "\n",
    "\n",
    "    node_attr.to_csv(folder_location+\"sc_\"+str(run_iter+1)+\"/\"+str(0)+ \"_node_attr.csv\")\n",
    "\n",
    "    df_3=cosine_similarity(node_attr.iloc[:,:8])\n",
    "    df_4=pd.DataFrame(df_3)\n",
    "    df_4.values[[np.arange(len(df_4))]*2] = np.nan\n",
    "    #mat_data.head()\n",
    "    edge_list_2=df_4.stack().reset_index()\n",
    "    edge_list_2.columns=['source','target','weight']\n",
    "    #edge_list_2.head()\n",
    "    edge_list_f=pd.merge(edge_list_df, edge_list_2,  how='left', left_on=['source','target'], right_on = ['source','target'])\n",
    "    #edge_list_f.head()\n",
    "    edge_list_f.drop('weight_x',axis=1,inplace=True)\n",
    "    edge_list_f.columns=['source','target','weight']\n",
    "\n",
    "    edge_list_f.to_csv(folder_location+\"sc_\"+str(run_iter+1)+\"/\"+\"initial_edge_attr.csv\")\n",
    "\n",
    "    ##### run simulation for all\n",
    "\n",
    "    print(\"@@@@@@@@@@@@@@@ -- \"+str(run_iter))\n",
    "    \"\"\"\n",
    "    w1=scn_params.iloc[run_iter,5]\n",
    "    w2=scn_params.iloc[run_iter,6]\n",
    "    w3=scn_params.iloc[run_iter,7]\n",
    "    w4=scn_params.iloc[run_iter,8]  \n",
    "    w5=scn_params.iloc[run_iter,9]  \n",
    "    \"\"\"\n",
    "    r_on=scn_params.iloc[run_iter,15]\n",
    "    m_on=scn_params.iloc[run_iter,16]\n",
    "    alpha1=scn_params.iloc[run_iter,17]\n",
    "    alpha2=scn_params.iloc[run_iter,18]\n",
    "    alpha3=scn_params.iloc[run_iter,19]\n",
    "    \n",
    "    if N==500:\n",
    "        num_sim=20\n",
    "    else:\n",
    "        num_sim=20\n",
    "    probs_mat=np.zeros((num_sim,N))\n",
    "\n",
    "    probs_mat2=np.zeros((((num_sim-1)*N)+1,N))\n",
    "\n",
    "    ## Initial node and edge attributes\n",
    "\n",
    "    node_attr=init_node_attrs_df.copy()\n",
    "    edge_list_df=init_edge_attrs_df.copy()\n",
    "    ################################################## simulation\n",
    "    simulation_continous(node_attr=node_attr,edge_list_df=edge_list_df,num_sim=num_sim,W=W,bs1=bs1,bs2=bs2,N=N,r_on=r_on,m_on=m_on,p_reg=p_reg,p_med=p_med,probs_mat=probs_mat,probs_mat2=probs_mat2,run_iter=run_iter,alpha1=alpha1,alpha2=alpha2,alpha3=alpha3,Tmp=Tmp,rgn=rgn,mcp=mcp)\n",
    "\n",
    "    lik_probs_mat=pd.DataFrame(probs_mat)\n",
    "    lik_probs_mat2=pd.DataFrame(probs_mat2)\n",
    "    \n",
    "    lik_probs_mat.to_csv(folder_location+\"sc_\"+str(run_iter+1)+\"/\"+\"lik_probs_mat.csv\")\n",
    "    lik_probs_mat2.to_csv(folder_location+\"sc_\"+str(run_iter+1)+\"/\"+\"lik_probs_mat2.csv\")\n",
    "    del lik_probs_mat\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b058e30f",
   "metadata": {},
   "source": [
    "### Running simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d141a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scenarios\n",
    "scn_params=pd.read_csv('simulation_log/testscenarios_uniform_parallel_W_alpha_new_sense_simple_new_missing_reg/testscenarios_uniform_parallel_W_alpha_new_sense_simple.csv')\n",
    "# Organizational data \n",
    "data_orgs=pd.read_csv('C:/Users/Krishna/OneDrive/Documents/IIM_R1/proj2/cosine_input.csv')\n",
    "\n",
    "folder_location='simulation_log/testscenarios_uniform_parallel_W_alpha_new_sense_simple_new_missing_reg/'\n",
    "\n",
    "\n",
    "from magic_functions import process_func\n",
    "frames_list = range(0,31)#range(0,8)\n",
    "with Pool(max_pool) as p:\n",
    "    pool_outputs = list(\n",
    "        tqdm(\n",
    "            p.imap(process_func,\n",
    "                   frames_list),\n",
    "            total=len(frames_list)\n",
    "        )\n",
    "    )    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
